{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T13:48:29.270854Z",
     "start_time": "2019-03-04T13:48:29.254284Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import keras\n",
    "\n",
    "from sklearn import random_projection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "\n",
    "# -------------- Helper Functions --------------\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize\n",
    "    e.g.\n",
    "    Input: 'It is a nice day. I am happy.'\n",
    "    Output: ['it', 'is', 'a', 'nice', 'day', 'i', 'am', 'happy']\n",
    "    '''\n",
    "    tokens = []\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and not word.isnumeric():\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_sequence(data, seq_length, vocab_dict):\n",
    "    '''\n",
    "    :param data: a list of words, type: list\n",
    "    :param seq_length: the length of sequences,, type: int\n",
    "    :param vocab_dict: a dict from words to indices, type: dict\n",
    "    return a dense sequence matrix whose elements are indices of words,\n",
    "    '''\n",
    "    data_matrix = np.zeros((len(data), seq_length), dtype=int)\n",
    "    for i, doc in enumerate(data):\n",
    "        for j, word in enumerate(doc):\n",
    "            # YOUR CODE HERE\n",
    "            if j == seq_length:\n",
    "                break\n",
    "            word_idx = vocab_dict.get(word, 1) # 1 means the unknown word\n",
    "            data_matrix[i, j] = word_idx\n",
    "    return data_matrix\n",
    "\n",
    "\n",
    "def read_data(file_name, input_length, vocab=None):\n",
    "    \"\"\"\n",
    "    https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "    df['words'] = df['text'].apply(tokenize)\n",
    "\n",
    "    if vocab is None:\n",
    "        vocab = set()\n",
    "        for i in range(len(df)):\n",
    "            for word in df.iloc[i]['words']:\n",
    "                vocab.add(word)\n",
    "    vocab_dict = dict()\n",
    "    vocab_dict['<pad>'] = 0 # 0 means the padding signal\n",
    "    vocab_dict['<unk>'] = 1 # 1 means the unknown word\n",
    "    vocab_size = 2\n",
    "    for v in vocab:\n",
    "        vocab_dict[v] = vocab_size\n",
    "        vocab_size += 1\n",
    "\n",
    "    data_matrix = get_sequence(df['words'], input_length, vocab_dict)\n",
    "    stars = df['stars'].apply(int) - 1\n",
    "    return df['review_id'], stars, data_matrix, vocab\n",
    "# ----------------- End of Helper Functions-----------------\n",
    "\n",
    "\n",
    "def load_data(input_length):\n",
    "     # Load training data and vocab\n",
    "    train_id_list, train_data_label, train_data_matrix, vocab = read_data(\"data/train.csv\", input_length)\n",
    "    K = max(train_data_label)+1  # labels begin with 0\n",
    "\n",
    "    # Load testing data\n",
    "    test_id_list, _, test_data_matrix, _ = read_data(\"data/test.csv\", input_length, vocab=vocab)\n",
    "    test_data_label = pd.read_csv(\"data/valid.csv\")['stars'] - 1\n",
    "    \n",
    "    print(\"Vocabulary Size:\", len(vocab))\n",
    "    print(\"Training Set Size:\", len(train_id_list))\n",
    "    print(\"Test Set Size:\", len(test_id_list))\n",
    "    print(\"Training Set Shape:\", train_data_matrix.shape)\n",
    "    print(\"Testing Set Shape:\", test_data_matrix.shape)\n",
    "\n",
    "    # Converts a class vector to binary class matrix.\n",
    "    # https://keras.io/utils/#to_categorical\n",
    "    train_data_label = keras.utils.to_categorical(train_data_label, num_classes=K)\n",
    "    test_data_label = keras.utils.to_categorical(test_data_label, num_classes=K)\n",
    "    return train_data_matrix, train_data_label, test_data_matrix, test_data_label, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T13:18:39.581282Z",
     "start_time": "2019-03-04T13:18:39.390783Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/valid.csv\")\n",
    "test_data_label = pd.read_csv(\"data/valid.csv\")['stars'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T13:18:56.861990Z",
     "start_time": "2019-03-04T13:18:56.857120Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_length = 30\n",
    "embedding_size = 100\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 0.1\n",
    "total_epoch = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T13:44:55.348740Z",
     "start_time": "2019-03-04T13:42:05.590679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 114655\n",
      "Training Set Size: 100000\n",
      "Test Set Size: 10000\n",
      "Training Set Shape: (100000, 30)\n",
      "Testing Set Shape: (10000, 30)\n"
     ]
    }
   ],
   "source": [
    "train_data_matrix, train_data_label, test_data_matrix, test_data_label, vocab = load_data(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T13:51:49.141903Z",
     "start_time": "2019-03-04T13:51:49.136643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data shape\n",
    "N = train_data_matrix.shape[0]\n",
    "K = train_data_label.shape[1]\n",
    "\n",
    "input_size = len(vocab) + 2\n",
    "output_size = K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 106s 1ms/step - loss: 1.3231 - acc: 0.4732\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 103s 1ms/step - loss: 1.0079 - acc: 0.5828\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 98s 981us/step - loss: 0.9426 - acc: 0.6082\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 98s 976us/step - loss: 0.9078 - acc: 0.6201\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 94s 937us/step - loss: 0.8767 - acc: 0.6333\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 100s 996us/step - loss: 0.8453 - acc: 0.6468\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 102s 1ms/step - loss: 0.8157 - acc: 0.6586\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 101s 1ms/step - loss: 0.7834 - acc: 0.6718\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 103s 1ms/step - loss: 0.7501 - acc: 0.6858\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 100s 1ms/step - loss: 0.7174 - acc: 0.7018\n",
      "100000/100000 [==============================] - 13s 129us/step\n",
      "10000/10000 [==============================] - 1s 124us/step\n",
      "Training Loss: 0.6220303770303727\n",
      " Training Accuracy: 0.7498799998760224\n",
      "Testng Loss: 3.1193943333625795\n",
      " Testing accuracy: 0.29440000087022783\n"
     ]
    }
   ],
   "source": [
    "# New model\n",
    "model = Sequential()\n",
    "        \n",
    "    \n",
    "# CNN\n",
    "model.add(Conv2D(filters=20, kernel_size=(5,5), padding='valid', input_shape=(28,28, 1), activation='tanh'))\n",
    "# First pooling layer\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "# Second convolutional layer\n",
    "model.add(Conv2D(filters=16, kernel_size=(5,5), padding='valid', activation='tanh'))\n",
    "# Second pooling layer \n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "# Flatten the 2D layer into 1D\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "# LSTM\n",
    "model = Sequential()\n",
    "# embedding layer and dropout\n",
    "model.add(Embedding(input_dim=input_size, output_dim=embedding_size, input_length=input_length))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(TimeDistributed(cnn, ...))\n",
    "model.add(LSTM(units=hidden_size))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(K, activation='softmax'))\n",
    "\n",
    "# SGD optimizer with momentum\n",
    "optimizer = SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# training\n",
    "model.fit(train_data_matrix, train_data_label, epochs=total_epoch, batch_size=batch_size)\n",
    "# testing\n",
    "train_score = model.evaluate(train_data_matrix, train_data_label, batch_size=batch_size)\n",
    "test_score = model.evaluate(test_data_matrix, test_data_label, batch_size=batch_size)\n",
    "\n",
    "print('Training Loss: {}\\n Training Accuracy: {}\\n'\n",
    "      'Testng Loss: {}\\n Testing accuracy: {}'.format(\n",
    "          train_score[0], train_score[1],\n",
    "          test_score[0], test_score[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting\n",
    "test_pre = model.predict(test_data_matrix, batch_size=batch_size)\n",
    "sub_df = pd.DataFrame()\n",
    "sub_df[\"review_id\"] = test_id_list\n",
    "sub_df[\"pre\"] = test_pre\n",
    "sub_df.to_csv(\"pre.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "271px",
    "left": "994px",
    "right": "0px",
    "top": "72px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
