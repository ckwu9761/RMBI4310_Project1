{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T17:00:15.362066Z",
     "start_time": "2019-03-08T17:00:10.678662Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:06:05.024982Z",
     "start_time": "2019-03-09T03:05:59.013259Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import keras\n",
    "import os\n",
    "import re\n",
    "\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "from sklearn import random_projection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten, Reshape\n",
    "from keras.layers import Conv1D, Conv2D, Convolution2D, MaxPool2D, MaxPooling2D, Flatten, Reshape, BatchNormalization, Concatenate\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l1, l2\n",
    "\n",
    "# from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n",
    "\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:06:08.506395Z",
     "start_time": "2019-03-09T03:06:07.387098Z"
    }
   },
   "outputs": [],
   "source": [
    "Train = pd.read_csv(\"data/train.csv\")\n",
    "Test = pd.read_csv(\"data/test.csv\")\n",
    "Valid = pd.read_csv(\"data/valid.csv\")\n",
    "\n",
    "#################################################################\n",
    "# Train = Train.iloc[0:10000,:]\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:06:08.610610Z",
     "start_time": "2019-03-09T03:06:08.584940Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dsvFUqrhytVmKXCW7bKNhA</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-03-10 01:24:07</td>\n",
       "      <td>0</td>\n",
       "      <td>58nqw-MdO6EDACPaKDM59Q</td>\n",
       "      <td>5.0</td>\n",
       "      <td>As a student, by back and neck are under const...</td>\n",
       "      <td>0</td>\n",
       "      <td>1NImHsg1kc76n_cQyTm0bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6YrO-hJNof4wsx4f0YQ8yg</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-12-22 17:06:23</td>\n",
       "      <td>0</td>\n",
       "      <td>eVeQtMGaB5tdCH0hKJwxKw</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Stayed here for a football game at University ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Oi0G3jFm2jtG2W02dZTdEQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LtyoPfxpvcF_9e9wMoUi0w</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-04-25 09:55:57</td>\n",
       "      <td>0</td>\n",
       "      <td>AHMHUbq0eAUOuOPFoeO5iw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Very good salads, generous portions. I either ...</td>\n",
       "      <td>1</td>\n",
       "      <td>4jbz7cOVuV_Q7v2b3pNrLw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QzvLnOqwH6BIY_jCOvzuQQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-06 04:18:48</td>\n",
       "      <td>0</td>\n",
       "      <td>8RDvwneSMsbpmi5UgGq60A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The experience I had with this company growing...</td>\n",
       "      <td>0</td>\n",
       "      <td>gJgPs0QXE587T9SIR6NCdQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tPaweigPsXacvQT8daYT4g</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-22 15:38:59</td>\n",
       "      <td>0</td>\n",
       "      <td>IboCoxoL0IFtJnsGPHCugg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Easiest furniture purchase, with a great price...</td>\n",
       "      <td>0</td>\n",
       "      <td>zp8XLBJmgw55ZTlp5raK7Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                 date  funny  \\\n",
       "0  dsvFUqrhytVmKXCW7bKNhA     0  2013-03-10 01:24:07      0   \n",
       "1  6YrO-hJNof4wsx4f0YQ8yg     0  2014-12-22 17:06:23      0   \n",
       "2  LtyoPfxpvcF_9e9wMoUi0w     0  2016-04-25 09:55:57      0   \n",
       "3  QzvLnOqwH6BIY_jCOvzuQQ     0  2017-04-06 04:18:48      0   \n",
       "4  tPaweigPsXacvQT8daYT4g     0  2017-03-22 15:38:59      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  58nqw-MdO6EDACPaKDM59Q    5.0   \n",
       "1  eVeQtMGaB5tdCH0hKJwxKw    3.0   \n",
       "2  AHMHUbq0eAUOuOPFoeO5iw    4.0   \n",
       "3  8RDvwneSMsbpmi5UgGq60A    1.0   \n",
       "4  IboCoxoL0IFtJnsGPHCugg    5.0   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  As a student, by back and neck are under const...       0   \n",
       "1  Stayed here for a football game at University ...       0   \n",
       "2  Very good salads, generous portions. I either ...       1   \n",
       "3  The experience I had with this company growing...       0   \n",
       "4  Easiest furniture purchase, with a great price...       0   \n",
       "\n",
       "                  user_id  \n",
       "0  1NImHsg1kc76n_cQyTm0bg  \n",
       "1  Oi0G3jFm2jtG2W02dZTdEQ  \n",
       "2  4jbz7cOVuV_Q7v2b3pNrLw  \n",
       "3  gJgPs0QXE587T9SIR6NCdQ  \n",
       "4  zp8XLBJmgw55ZTlp5raK7Q  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:06:08.695916Z",
     "start_time": "2019-03-09T03:06:08.676373Z"
    }
   },
   "outputs": [],
   "source": [
    "# Label\n",
    "Train_y = Train['stars']\n",
    "Test_y = Valid['stars']\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "y_ohe_train = ohe.fit_transform(Train_y.values.reshape(-1, 1))\n",
    "y_ohe_test = ohe.fit_transform(Test_y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:06:08.785168Z",
     "start_time": "2019-03-09T03:06:08.776309Z"
    }
   },
   "outputs": [],
   "source": [
    "train = Train[['text','stars']]\n",
    "test = Valid[['text','stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:06:08.898794Z",
     "start_time": "2019-03-09T03:06:08.887122Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As a student, by back and neck are under const...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stayed here for a football game at University ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very good salads, generous portions. I either ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The experience I had with this company growing...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Easiest furniture purchase, with a great price...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars\n",
       "0  As a student, by back and neck are under const...    5.0\n",
       "1  Stayed here for a football game at University ...    3.0\n",
       "2  Very good salads, generous portions. I either ...    4.0\n",
       "3  The experience I had with this company growing...    1.0\n",
       "4  Easiest furniture purchase, with a great price...    5.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:06:08.985275Z",
     "start_time": "2019-03-09T03:06:08.978294Z"
    }
   },
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "def features(df):\n",
    "    ## Number of Words\n",
    "    df['word_count'] = df['text'].apply(lambda x: len(str(x).split(\" \"))) \n",
    "    ## Number of characters\n",
    "    df['char_count'] = df['text'].str.len() \n",
    "    ## Average Word Length\n",
    "    df['avg_word'] = df['text'].apply(lambda x: avg_word(x))\n",
    "    ## Number of stopwords\n",
    "    df['stopwords'] = df['text'].apply(lambda x: len([x for x in x.split() if x in stop_words]))\n",
    "    ## Number of special characters\n",
    "    df['hastags'] = df['text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "    ##Number of numerics\n",
    "    df['numerics'] = df['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "    ## Number of Uppercase words\n",
    "    df['upper'] = df['text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:06:25.767895Z",
     "start_time": "2019-03-09T03:06:09.078826Z"
    }
   },
   "outputs": [],
   "source": [
    "features(train)\n",
    "features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:06:25.885042Z",
     "start_time": "2019-03-09T03:06:25.870338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>hastags</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As a student, by back and neck are under const...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>48</td>\n",
       "      <td>295</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stayed here for a football game at University ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37</td>\n",
       "      <td>218</td>\n",
       "      <td>4.918919</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very good salads, generous portions. I either ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19</td>\n",
       "      <td>106</td>\n",
       "      <td>4.631579</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The experience I had with this company growing...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>134</td>\n",
       "      <td>716</td>\n",
       "      <td>4.343284</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Easiest furniture purchase, with a great price...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19</td>\n",
       "      <td>122</td>\n",
       "      <td>5.473684</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars  word_count  \\\n",
       "0  As a student, by back and neck are under const...    5.0          48   \n",
       "1  Stayed here for a football game at University ...    3.0          37   \n",
       "2  Very good salads, generous portions. I either ...    4.0          19   \n",
       "3  The experience I had with this company growing...    1.0         134   \n",
       "4  Easiest furniture purchase, with a great price...    5.0          19   \n",
       "\n",
       "   char_count  avg_word  stopwords  hastags  numerics  upper  \n",
       "0         295  5.166667         14        0         0      2  \n",
       "1         218  4.918919         13        0         0      0  \n",
       "2         106  4.631579          4        0         0      2  \n",
       "3         716  4.343284         55        0         2      7  \n",
       "4         122  5.473684          6        0         0      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:30:13.246419Z",
     "start_time": "2019-03-09T03:30:13.241403Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_processing(df):\n",
    "    \n",
    "    ## Lower case\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    ## Removing Punctuation\n",
    "    df['text'] = df['text'].str.replace('[^\\w\\s]','')\n",
    "    ## Removing Stopwords\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n",
    "    ## Spelling correction (time-required)\n",
    "#     df['text'] = df['text'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:30:19.378920Z",
     "start_time": "2019-03-09T03:30:13.734329Z"
    }
   },
   "outputs": [],
   "source": [
    "pre_processing(train)\n",
    "pre_processing(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:30:19.496091Z",
     "start_time": "2019-03-09T03:30:19.484585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        student back neck constant strain dr serrick m...\n",
       "1        stayed football game university phoenix stadiu...\n",
       "2        good salads generous portions either get mexic...\n",
       "3        experience company growing awesome least memor...\n",
       "4        easiest furniture purchase great price deliver...\n",
       "5        great hotel clean dont like pay 34 resort fee ...\n",
       "6        café coffee actually important worse version p...\n",
       "7        impressed ever wanting try place finally night...\n",
       "8        first night vegas wanted something simple dinn...\n",
       "9        21 reviews 12 1star variety coincidence shock ...\n",
       "10       place used amazing amazing service amazing foo...\n",
       "11       best fried chicken ever town concert stumbled ...\n",
       "12       early bird specials 11am 10am forget soup buns...\n",
       "13       really like place definitely perfect play area...\n",
       "14       little disappointed 20oz bonein ribeye 50 came...\n",
       "15       whoever picked name convenient little restaura...\n",
       "16       extremely extremely super nice staff owners mo...\n",
       "17       got stir fried sticky rice grandma huge box ac...\n",
       "18       really disappointing limited selection product...\n",
       "19       glad gave guys second chance first time ordere...\n",
       "20       best customer service ever experienced vendor ...\n",
       "21       ordered go waited seems like lot takeout order...\n",
       "22       clean well organized friendly staff produce de...\n",
       "23       love concept behind everybody integrative mass...\n",
       "24       1 best healthy food downtown 2 food made 3 fre...\n",
       "25       place awesome food delicious staff helpful res...\n",
       "26       cookies pretty good experience first time seei...\n",
       "27       manicure pedicure done came home checked manic...\n",
       "28       called different places get estimates 2 units ...\n",
       "29       hands absolute favorite bar las vegas dont oft...\n",
       "                               ...                        \n",
       "99970    ive 9091 twice one week say good potato string...\n",
       "99971    ive many times groups time food best mediocre ...\n",
       "99972    many portuguese restaurants many cities excite...\n",
       "99973    pleasure visit kings fish house tempe marketpl...\n",
       "99974    scam theyre scam went get root canal today cal...\n",
       "99975    dr nemanic helping 2 displaced ribs disc quick...\n",
       "99976    wonderful experience overall guys saved skin d...\n",
       "99977    came breakfast sunday cute trendy place enjoy ...\n",
       "99978    one two great restaurants brecksville havent t...\n",
       "99979    lets go straight point drinks good service nee...\n",
       "99980    would give negative number offered gal airport...\n",
       "99981    great quality sushi fun occasions group romant...\n",
       "99982    much urging italian friends west valley finall...\n",
       "99983    nothing better warm sandwich jersey mikes hung...\n",
       "99984    person cant feel home harolds theres hope grea...\n",
       "99985    expecting bit main library city size charlotte...\n",
       "99986    bento box really good excellent service clean ...\n",
       "99987    business professional tempe dont like play aro...\n",
       "99988    clean establishment employees really nice chic...\n",
       "99989    looking amazing independent authentic deliciou...\n",
       "99990    fries gyro anything better gyro good size tast...\n",
       "99991    lgo unique delicious pizzas fabulous gelato ne...\n",
       "99992    favorite breakfast spots love chicken fried st...\n",
       "99993    good decor lunch special l10 spicy tofu pork m...\n",
       "99994    ate last thursday coworker sat bar ordered wes...\n",
       "99995    10 rum coke huh right fun place sit people wat...\n",
       "99996    love love love place fabulous lunch specials g...\n",
       "99997    unsuspecting restaurant great casual place cat...\n",
       "99998    copa copa cabana fell love sang barry manilow ...\n",
       "99999    love love love x 1000 place lived expectations...\n",
       "Name: text, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rare words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:30:19.641819Z",
     "start_time": "2019-03-09T03:30:19.597222Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:30:22.319328Z",
     "start_time": "2019-03-09T03:30:19.762876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lancasters       1\n",
       "heinekens        1\n",
       "opsayo           1\n",
       "tablelike        1\n",
       "madisoncom       1\n",
       "mosse            1\n",
       "hamerschlag      1\n",
       "stimulatingly    1\n",
       "businesstype     1\n",
       "juuuuuuust       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(df['text']).split()).value_counts()[-10:]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:30:25.211036Z",
     "start_time": "2019-03-09T03:30:22.429549Z"
    }
   },
   "outputs": [],
   "source": [
    "freq = list(freq.index)\n",
    "train['text'] = train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "test['text'] = test['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:30:25.324398Z",
     "start_time": "2019-03-09T03:30:25.319558Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_lemm(df):\n",
    "    train['text'] = train['text'].apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split()]))\n",
    "    train['text'] = train['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:40:11.821093Z",
     "start_time": "2019-03-09T03:30:25.442841Z"
    }
   },
   "outputs": [],
   "source": [
    "stem_lemm(train)\n",
    "stem_lemm(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:40:42.597330Z",
     "start_time": "2019-03-09T03:40:12.018715Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=100, lowercase=True, analyzer='word',\n",
    "                        stop_words='english', ngram_range=(1, 2))\n",
    "\n",
    "train_vect = tfidf.fit_transform(train['text'])\n",
    "test_vect = tfidf.fit_transform(test['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:49:55.036064Z",
     "start_time": "2019-03-09T03:49:39.846687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 92161 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(100)\n",
    "tokenizer.fit_on_texts(train['text'])\n",
    "sequences = tokenizer.texts_to_sequences(train['text'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:40:42.799304Z",
     "start_time": "2019-03-09T03:40:42.740310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amazing</th>\n",
       "      <th>area</th>\n",
       "      <th>asked</th>\n",
       "      <th>awesome</th>\n",
       "      <th>bad</th>\n",
       "      <th>bar</th>\n",
       "      <th>best</th>\n",
       "      <th>better</th>\n",
       "      <th>big</th>\n",
       "      <th>bit</th>\n",
       "      <th>...</th>\n",
       "      <th>vegas</th>\n",
       "      <th>wait</th>\n",
       "      <th>want</th>\n",
       "      <th>wanted</th>\n",
       "      <th>wasnt</th>\n",
       "      <th>way</th>\n",
       "      <th>went</th>\n",
       "      <th>work</th>\n",
       "      <th>worth</th>\n",
       "      <th>youre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.179496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.171823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   amazing  area  asked  awesome  bad  bar  best  better  big  bit    ...     \\\n",
       "0      0.0   0.0    0.0      0.0  0.0  0.0   0.0     0.0  0.0  0.0    ...      \n",
       "1      0.0   0.0    0.0      0.0  0.0  0.0   0.0     0.0  0.0  0.0    ...      \n",
       "2      0.0   0.0    0.0      0.0  0.0  0.0   0.0     0.0  0.0  0.0    ...      \n",
       "3      0.0   0.0    0.0      0.0  0.0  0.0   0.0     0.0  0.0  0.0    ...      \n",
       "4      0.0   0.0    0.0      0.0  0.0  0.0   0.0     0.0  0.0  0.0    ...      \n",
       "\n",
       "   vegas  wait  want  wanted  wasnt       way  went  work  worth     youre  \n",
       "0    0.0   0.0   0.0     0.0    0.0  0.000000   0.0   0.0    0.0  0.000000  \n",
       "1    0.0   0.0   0.0     0.0    0.0  0.000000   0.0   0.0    0.0  0.000000  \n",
       "2    0.0   0.0   0.0     0.0    0.0  0.000000   0.0   0.0    0.0  0.000000  \n",
       "3    0.0   0.0   0.0     0.0    0.0  0.179496   0.0   0.0    0.0  0.171823  \n",
       "4    0.0   0.0   0.0     0.0    0.0  0.000000   0.0   0.0    0.0  0.000000  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the Model as a pandas DataFrame\n",
    "feature_names = tfidf.get_feature_names()\n",
    "train_vectdf = pd.DataFrame(train_vect.toarray(), columns = feature_names)\n",
    "train_vectdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:40:49.515204Z",
     "start_time": "2019-03-09T03:40:42.985473Z"
    }
   },
   "outputs": [],
   "source": [
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "train_bow = bow.fit_transform(train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T04:05:22.291693Z",
     "start_time": "2019-03-09T04:04:31.968461Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_input_file = '../glove.6B.100d.txt'\n",
    "word2vec_output_file = '../glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "from gensim.models import KeyedVectors \n",
    "\n",
    "filename = '../glove.6B.100d.txt.word2vec'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T04:05:22.784181Z",
     "start_time": "2019-03-09T04:05:22.780964Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 1000\n",
    "MAX_SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T04:05:23.250924Z",
     "start_time": "2019-03-09T04:05:23.243561Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = embeddings_index['a'].shape[0]\n",
    "num_words = 400000\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T04:19:20.110731Z",
     "start_time": "2019-03-09T04:19:20.053245Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'servic' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-9adb8c84562b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# This references the loaded embeddings dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0membedding_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# words not found in embedding index will be all-zeros.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'servic' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "for word, i in word_index.items():\n",
    "    if i > 10:\n",
    "        continue\n",
    "    # This references the loaded embeddings dictionary\n",
    "    embedding_vector = embeddings_index[word]\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T04:27:37.642405Z",
     "start_time": "2019-03-09T04:27:37.637449Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_length = 100\n",
    "embedding_size = 100\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "dropout_rate = 0.5\n",
    "filters = 100\n",
    "kernel_sizes = [3, 6, 9]\n",
    "padding = 'valid'\n",
    "activation = 'relu'\n",
    "strides = 1\n",
    "pool_size = 2\n",
    "learning_rate = 0.1\n",
    "total_epoch = 10\n",
    "output_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T04:27:38.163859Z",
     "start_time": "2019-03-09T04:27:38.159172Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating Callbacks\n",
    "# ModelCheckpoints is used to save the model after every epoch\n",
    "# EarlyStopping is used to stop training when the validation loss has not improved after 2 epochs\n",
    "# Tensorboard is used tovisualize dynamic graphs of the training and test metrics\n",
    "cbks = [callbacks.ModelCheckpoint(filepath='./checkpoint_model.h5', monitor='val_loss', save_best_only=True),\n",
    "            callbacks.EarlyStopping(monitor='val_loss', patience=2),callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T05:22:24.852277Z",
     "start_time": "2019-03-09T05:22:24.405076Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 100, 100)          10000000  \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 100, 100)          30100     \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 100, 100)          60100     \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 100, 100)          90100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1, 100)            0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 10,261,205\n",
      "Trainable params: 10,261,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Final Model Architecture\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=100000, output_dim=embedding_size,\n",
    "                    input_length=input_length, dropout=0.2))\n",
    "model.add(Conv1D(filters=filters, kernel_size=3, padding='same',\n",
    "                 activation='relu', activity_regularizer=l1(0.001)))\n",
    "model.add(Conv1D(filters=filters, kernel_size=6, padding='same',\n",
    "                 activation='relu', activity_regularizer=l1(0.01)))\n",
    "model.add(Conv1D(filters=filters, kernel_size=9, padding='same',\n",
    "                 activation='relu', kernel_regularizer=l2(1.0)))\n",
    "model.add(MaxPooling1D(pool_size=((input_length-kernel_sizes[2])//strides+1)))\n",
    "# 1 layer of 100 units in the hidden layers of the LSTM cells\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(output_size, activation='softmax',\n",
    "                activity_regularizer=l1(0.001)))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-09T05:28:21.420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  6976/100000 [=>............................] - ETA: 9:54 - loss: 1.4575 - acc: 0.4438"
     ]
    }
   ],
   "source": [
    "model.fit(train_vect, y_ohe_train, epochs=10,\n",
    "          verbose=1, batch_size=32, callbacks=cbks)\n",
    "\n",
    "train_score = model.evaluate(\n",
    "    train_vect, y_ohe_train, batch_size=batch_size)\n",
    "test_score = model.evaluate(\n",
    "    test_vect, y_ohe_test, batch_size=batch_size)\n",
    "\n",
    "print('Training Loss: {}\\n Training Accuracy: {}\\n'\n",
    "      'Testng Loss: {}\\n Testing accuracy: {}'.format(\n",
    "          train_score[0], train_score[1],\n",
    "          test_score[0], test_score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T05:26:59.666428Z",
     "start_time": "2019-03-09T05:26:59.574181Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Sequential' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-d758278a1aa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfig1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Sequential' object is not subscriptable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visualizing Loss\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "%matplotlib inline\n",
    "\n",
    "fig1 = plt.figure()\n",
    "plt.plot(model.model['loss'],'r',linewidth=3.0)\n",
    "plt.legend(['Training Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('CNN Loss',fontsize=16)\n",
    "fig1.savefig('loss_cnn.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing accuracy\n",
    "\n",
    "fig2 = plt.figure()\n",
    "plt.plot(model.model['acc'], 'r', linewidth=3.0)\n",
    "plt.legend(['Training Accuracy'], fontsize=18)\n",
    "plt.xlabel('Epochs ', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.title('CNN Accuracy', fontsize=16)\n",
    "fig2.savefig('accuracy_cnn.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": "9999",
    "lenType": "9999",
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "370px",
    "left": "977px",
    "right": "7px",
    "top": "75px",
    "width": "388px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
