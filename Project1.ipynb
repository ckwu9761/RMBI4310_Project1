{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T13:48:29.270854Z",
     "start_time": "2019-03-04T13:48:29.254284Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import keras\n",
    "\n",
    "from sklearn import random_projection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "\n",
    "# -------------- Helper Functions --------------\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize\n",
    "    e.g.\n",
    "    Input: 'It is a nice day. I am happy.'\n",
    "    Output: ['it', 'is', 'a', 'nice', 'day', 'i', 'am', 'happy']\n",
    "    '''\n",
    "    tokens = []\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and not word.isnumeric():\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_sequence(data, seq_length, vocab_dict):\n",
    "    '''\n",
    "    :param data: a list of words, type: list\n",
    "    :param seq_length: the length of sequences,, type: int\n",
    "    :param vocab_dict: a dict from words to indices, type: dict\n",
    "    return a dense sequence matrix whose elements are indices of words,\n",
    "    '''\n",
    "    data_matrix = np.zeros((len(data), seq_length), dtype=int)\n",
    "    for i, doc in enumerate(data):\n",
    "        for j, word in enumerate(doc):\n",
    "            # YOUR CODE HERE\n",
    "            if j == seq_length:\n",
    "                break\n",
    "            word_idx = vocab_dict.get(word, 1) # 1 means the unknown word\n",
    "            data_matrix[i, j] = word_idx\n",
    "    return data_matrix\n",
    "\n",
    "\n",
    "def read_data(file_name, input_length, vocab=None):\n",
    "    \"\"\"\n",
    "    https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "    df['words'] = df['text'].apply(tokenize)\n",
    "\n",
    "    if vocab is None:\n",
    "        vocab = set()\n",
    "        for i in range(len(df)):\n",
    "            for word in df.iloc[i]['words']:\n",
    "                vocab.add(word)\n",
    "    vocab_dict = dict()\n",
    "    vocab_dict['<pad>'] = 0 # 0 means the padding signal\n",
    "    vocab_dict['<unk>'] = 1 # 1 means the unknown word\n",
    "    vocab_size = 2\n",
    "    for v in vocab:\n",
    "        vocab_dict[v] = vocab_size\n",
    "        vocab_size += 1\n",
    "\n",
    "    data_matrix = get_sequence(df['words'], input_length, vocab_dict)\n",
    "    stars = df['stars'].apply(int) - 1\n",
    "    return df['review_id'], stars, data_matrix, vocab\n",
    "# ----------------- End of Helper Functions-----------------\n",
    "\n",
    "\n",
    "def load_data(input_length):\n",
    "     # Load training data and vocab\n",
    "    train_id_list, train_data_label, train_data_matrix, vocab = read_data(\"data/train.csv\", input_length)\n",
    "    K = max(train_data_label)+1  # labels begin with 0\n",
    "\n",
    "    # Load testing data\n",
    "    test_id_list, _, test_data_matrix, _ = read_data(\"data/test.csv\", input_length, vocab=vocab)\n",
    "    test_data_label = pd.read_csv(\"data/valid.csv\")['stars'] - 1\n",
    "    \n",
    "    print(\"Vocabulary Size:\", len(vocab))\n",
    "    print(\"Training Set Size:\", len(train_id_list))\n",
    "    print(\"Test Set Size:\", len(test_id_list))\n",
    "    print(\"Training Set Shape:\", train_data_matrix.shape)\n",
    "    print(\"Testing Set Shape:\", test_data_matrix.shape)\n",
    "\n",
    "    # Converts a class vector to binary class matrix.\n",
    "    # https://keras.io/utils/#to_categorical\n",
    "    train_data_label = keras.utils.to_categorical(train_data_label, num_classes=K)\n",
    "    test_data_label = keras.utils.to_categorical(test_data_label, num_classes=K)\n",
    "    return train_data_matrix, train_data_label, test_data_matrix, test_data_label, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T13:18:39.581282Z",
     "start_time": "2019-03-04T13:18:39.390783Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/valid.csv\")\n",
    "test_data_label = pd.read_csv(\"data/valid.csv\")['stars'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T13:18:56.861990Z",
     "start_time": "2019-03-04T13:18:56.857120Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_length = 30\n",
    "embedding_size = 100\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 0.1\n",
    "total_epoch = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T13:44:55.348740Z",
     "start_time": "2019-03-04T13:42:05.590679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 114655\n",
      "Training Set Size: 100000\n",
      "Test Set Size: 10000\n",
      "Training Set Shape: (100000, 30)\n",
      "Testing Set Shape: (10000, 30)\n"
     ]
    }
   ],
   "source": [
    "train_data_matrix, train_data_label, test_data_matrix, test_data_label, vocab = load_data(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T13:51:49.141903Z",
     "start_time": "2019-03-04T13:51:49.136643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data shape\n",
    "N = train_data_matrix.shape[0]\n",
    "K = train_data_label.shape[1]\n",
    "\n",
    "input_size = len(vocab) + 2\n",
    "output_size = K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T13:52:21.387641Z",
     "start_time": "2019-03-04T13:52:21.362095Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-5287b5da6850>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# First pooling layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPool2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                  \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                  **kwargs):\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 batch_input_shape = (\n\u001b[0;32m--> 147\u001b[0;31m                     batch_size,) + tuple(kwargs['input_shape'])\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_input_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# New model\n",
    "model = Sequential()\n",
    "        \n",
    "    \n",
    "# CNN\n",
    "model.add(Conv2D(filters=20, kernel_size=(5,5), padding='valid', input_shape=K, activation='tanh'))\n",
    "# First pooling layer\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "# Second convolutional layer\n",
    "model.add(Conv2D(filters=16, kernel_size=(5,5), padding='valid', activation='tanh'))\n",
    "# Second pooling layer \n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "# Flatten the 2D layer into 1D\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully-connected layer\n",
    "model.add(Dense(120, activation='tanh'))\n",
    "model.add(Dense(84, activation='tanh'))\n",
    "\n",
    "# Output 5 labels\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# training\n",
    "model.fit(train_data_matrix, train_data_label, epochs=total_epoch, batch_size=batch_size)\n",
    "# testing\n",
    "train_score = model.evaluate(train_data_matrix, train_data_label, batch_size=batch_size)\n",
    "test_score = model.evaluate(test_data_matrix, test_data_label, batch_size=batch_size)\n",
    "\n",
    "print('Training Loss: {}\\n Training Accuracy: {}\\n'\n",
    "      'Testng Loss: {}\\n Testing accuracy: {}'.format(\n",
    "          train_score[0], train_score[1],\n",
    "          test_score[0], test_score[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 106s 1ms/step - loss: 1.3231 - acc: 0.4732\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 103s 1ms/step - loss: 1.0079 - acc: 0.5828\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 98s 981us/step - loss: 0.9426 - acc: 0.6082\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 98s 976us/step - loss: 0.9078 - acc: 0.6201\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 94s 937us/step - loss: 0.8767 - acc: 0.6333\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 100s 996us/step - loss: 0.8453 - acc: 0.6468\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 102s 1ms/step - loss: 0.8157 - acc: 0.6586\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 101s 1ms/step - loss: 0.7834 - acc: 0.6718\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 103s 1ms/step - loss: 0.7501 - acc: 0.6858\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 100s 1ms/step - loss: 0.7174 - acc: 0.7018\n",
      "100000/100000 [==============================] - 13s 129us/step\n",
      "10000/10000 [==============================] - 1s 124us/step\n",
      "Training Loss: 0.6220303770303727\n",
      " Training Accuracy: 0.7498799998760224\n",
      "Testng Loss: 3.1193943333625795\n",
      " Testing accuracy: 0.29440000087022783\n"
     ]
    }
   ],
   "source": [
    "# New model\n",
    "model = Sequential()\n",
    "        \n",
    "    \n",
    "# CNN\n",
    "model.add(Conv2D(filters=20, kernel_size=(5,5), padding='valid', input_shape=(28,28, 1), activation='tanh'))\n",
    "# First pooling layer\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "# Second convolutional layer\n",
    "model.add(Conv2D(filters=16, kernel_size=(5,5), padding='valid', activation='tanh'))\n",
    "# Second pooling layer \n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "# Flatten the 2D layer into 1D\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "# LSTM\n",
    "model = Sequential()\n",
    "# embedding layer and dropout\n",
    "model.add(Embedding(input_dim=input_size, output_dim=embedding_size, input_length=input_length))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(TimeDistributed(cnn, ...))\n",
    "model.add(LSTM(units=hidden_size))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(K, activation='softmax'))\n",
    "\n",
    "# SGD optimizer with momentum\n",
    "optimizer = SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# training\n",
    "model.fit(train_data_matrix, train_data_label, epochs=total_epoch, batch_size=batch_size)\n",
    "# testing\n",
    "train_score = model.evaluate(train_data_matrix, train_data_label, batch_size=batch_size)\n",
    "test_score = model.evaluate(test_data_matrix, test_data_label, batch_size=batch_size)\n",
    "\n",
    "print('Training Loss: {}\\n Training Accuracy: {}\\n'\n",
    "      'Testng Loss: {}\\n Testing accuracy: {}'.format(\n",
    "          train_score[0], train_score[1],\n",
    "          test_score[0], test_score[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting\n",
    "test_pre = model.predict(test_data_matrix, batch_size=batch_size)\n",
    "sub_df = pd.DataFrame()\n",
    "sub_df[\"review_id\"] = test_id_list\n",
    "sub_df[\"pre\"] = test_pre\n",
    "sub_df.to_csv(\"pre.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "271px",
    "left": "994px",
    "right": "0px",
    "top": "72px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
