{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T14:35:12.306433Z",
     "start_time": "2019-03-10T14:35:08.259785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting untokenize\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/46/e7cea8159199096e1df52da20a57a6665da80c37fb8aeb848a3e47442c32/untokenize-0.1.1.tar.gz\n",
      "Building wheels for collected packages: untokenize\n",
      "  Running setup.py bdist_wheel for untokenize ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/u25073/.cache/pip/wheels/f4/a0/63/012f35213063bcd1d92a1bb5f4a570fec73e2a0a335a9a7295\n",
      "Successfully built untokenize\n",
      "Installing collected packages: untokenize\n",
      "Successfully installed untokenize-0.1.1\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install untokenize --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T03:27:26.183665Z",
     "start_time": "2019-03-11T03:27:10.521985Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import keras\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from untokenize import untokenize\n",
    "\n",
    "from sklearn import random_projection\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "\n",
    "from mosestokenizer import MosesTokenizer, MosesDetokenizer\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten, Reshape\n",
    "from keras.layers import Conv1D, Conv2D, Convolution2D, MaxPool2D, MaxPooling2D, Flatten, Reshape, BatchNormalization, Concatenate\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l1, l2\n",
    "\n",
    "# from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n",
    "\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:50:51.032213Z",
     "start_time": "2019-03-09T13:50:49.983938Z"
    }
   },
   "outputs": [],
   "source": [
    "Train = pd.read_csv(\"./data/train.csv\")\n",
    "Test = pd.read_csv(\"./data/test.csv\")\n",
    "Valid = pd.read_csv(\"./data/valid.csv\")\n",
    "\n",
    "#################################################################\n",
    "# Train = Train.iloc[0:10000,:]\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:50:51.603917Z",
     "start_time": "2019-03-09T13:50:51.582829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dsvFUqrhytVmKXCW7bKNhA</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-03-10 01:24:07</td>\n",
       "      <td>0</td>\n",
       "      <td>58nqw-MdO6EDACPaKDM59Q</td>\n",
       "      <td>5.0</td>\n",
       "      <td>As a student, by back and neck are under const...</td>\n",
       "      <td>0</td>\n",
       "      <td>1NImHsg1kc76n_cQyTm0bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6YrO-hJNof4wsx4f0YQ8yg</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-12-22 17:06:23</td>\n",
       "      <td>0</td>\n",
       "      <td>eVeQtMGaB5tdCH0hKJwxKw</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Stayed here for a football game at University ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Oi0G3jFm2jtG2W02dZTdEQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LtyoPfxpvcF_9e9wMoUi0w</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-04-25 09:55:57</td>\n",
       "      <td>0</td>\n",
       "      <td>AHMHUbq0eAUOuOPFoeO5iw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Very good salads, generous portions. I either ...</td>\n",
       "      <td>1</td>\n",
       "      <td>4jbz7cOVuV_Q7v2b3pNrLw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QzvLnOqwH6BIY_jCOvzuQQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-06 04:18:48</td>\n",
       "      <td>0</td>\n",
       "      <td>8RDvwneSMsbpmi5UgGq60A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The experience I had with this company growing...</td>\n",
       "      <td>0</td>\n",
       "      <td>gJgPs0QXE587T9SIR6NCdQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tPaweigPsXacvQT8daYT4g</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-22 15:38:59</td>\n",
       "      <td>0</td>\n",
       "      <td>IboCoxoL0IFtJnsGPHCugg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Easiest furniture purchase, with a great price...</td>\n",
       "      <td>0</td>\n",
       "      <td>zp8XLBJmgw55ZTlp5raK7Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                 date  funny  \\\n",
       "0  dsvFUqrhytVmKXCW7bKNhA     0  2013-03-10 01:24:07      0   \n",
       "1  6YrO-hJNof4wsx4f0YQ8yg     0  2014-12-22 17:06:23      0   \n",
       "2  LtyoPfxpvcF_9e9wMoUi0w     0  2016-04-25 09:55:57      0   \n",
       "3  QzvLnOqwH6BIY_jCOvzuQQ     0  2017-04-06 04:18:48      0   \n",
       "4  tPaweigPsXacvQT8daYT4g     0  2017-03-22 15:38:59      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  58nqw-MdO6EDACPaKDM59Q    5.0   \n",
       "1  eVeQtMGaB5tdCH0hKJwxKw    3.0   \n",
       "2  AHMHUbq0eAUOuOPFoeO5iw    4.0   \n",
       "3  8RDvwneSMsbpmi5UgGq60A    1.0   \n",
       "4  IboCoxoL0IFtJnsGPHCugg    5.0   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  As a student, by back and neck are under const...       0   \n",
       "1  Stayed here for a football game at University ...       0   \n",
       "2  Very good salads, generous portions. I either ...       1   \n",
       "3  The experience I had with this company growing...       0   \n",
       "4  Easiest furniture purchase, with a great price...       0   \n",
       "\n",
       "                  user_id  \n",
       "0  1NImHsg1kc76n_cQyTm0bg  \n",
       "1  Oi0G3jFm2jtG2W02dZTdEQ  \n",
       "2  4jbz7cOVuV_Q7v2b3pNrLw  \n",
       "3  gJgPs0QXE587T9SIR6NCdQ  \n",
       "4  zp8XLBJmgw55ZTlp5raK7Q  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T14:53:36.584871Z",
     "start_time": "2019-03-09T14:53:36.320551Z"
    }
   },
   "outputs": [],
   "source": [
    "train = Train[['text','stars']]\n",
    "test = Valid[['text','stars']]\n",
    "df = pd.concat([train, test])\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:51:18.331341Z",
     "start_time": "2019-03-09T13:51:18.325943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data  (100000,)\n",
      "train_labels  (100000,)\n",
      "____________________________________________________________________________________________________\n",
      "test_data  (10000,)\n",
      "test_labels  (10000,)\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"train_data \", train['text'].shape)\n",
    "print(\"train_labels \", train['stars'].shape)\n",
    "print(\"_\"*100)\n",
    "print(\"test_data \", test['text'].shape)\n",
    "print(\"test_labels \", test['stars'].shape)\n",
    "print(\"_\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T14:53:38.646150Z",
     "start_time": "2019-03-09T14:53:38.635291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As a student, by back and neck are under const...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stayed here for a football game at University ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very good salads, generous portions. I either ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The experience I had with this company growing...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Easiest furniture purchase, with a great price...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars\n",
       "0  As a student, by back and neck are under const...    5.0\n",
       "1  Stayed here for a football game at University ...    3.0\n",
       "2  Very good salads, generous portions. I either ...    4.0\n",
       "3  The experience I had with this company growing...    1.0\n",
       "4  Easiest furniture purchase, with a great price...    5.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:50:57.371728Z",
     "start_time": "2019-03-09T13:50:57.365226Z"
    }
   },
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "def features(df):\n",
    "    ## Number of Words\n",
    "    df['word_count'] = df['text'].apply(lambda x: len(str(x).split(\" \"))) \n",
    "    ## Number of characters\n",
    "    df['char_count'] = df['text'].str.len() \n",
    "    ## Average Word Length\n",
    "    df['avg_word'] = df['text'].apply(lambda x: avg_word(x))\n",
    "    ## Number of stopwords\n",
    "    df['stopwords'] = df['text'].apply(lambda x: len([x for x in x.split() if x in stop_words]))\n",
    "    ## Number of special characters\n",
    "    df['hastags'] = df['text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "    ##Number of numerics\n",
    "    df['numerics'] = df['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "    ## Number of Uppercase words\n",
    "    df['upper'] = df['text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T14:55:17.858840Z",
     "start_time": "2019-03-09T14:55:01.701293Z"
    }
   },
   "outputs": [],
   "source": [
    "features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T14:55:19.611792Z",
     "start_time": "2019-03-09T14:55:19.594951Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>hastags</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As a student, by back and neck are under const...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>48</td>\n",
       "      <td>295</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stayed here for a football game at University ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37</td>\n",
       "      <td>218</td>\n",
       "      <td>4.918919</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very good salads, generous portions. I either ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19</td>\n",
       "      <td>106</td>\n",
       "      <td>4.631579</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The experience I had with this company growing...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>134</td>\n",
       "      <td>716</td>\n",
       "      <td>4.343284</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Easiest furniture purchase, with a great price...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19</td>\n",
       "      <td>122</td>\n",
       "      <td>5.473684</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars  word_count  \\\n",
       "0  As a student, by back and neck are under const...    5.0          48   \n",
       "1  Stayed here for a football game at University ...    3.0          37   \n",
       "2  Very good salads, generous portions. I either ...    4.0          19   \n",
       "3  The experience I had with this company growing...    1.0         134   \n",
       "4  Easiest furniture purchase, with a great price...    5.0          19   \n",
       "\n",
       "   char_count  avg_word  stopwords  hastags  numerics  upper  \n",
       "0         295  5.166667         14        0         0      2  \n",
       "1         218  4.918919         13        0         0      0  \n",
       "2         106  4.631579          4        0         0      2  \n",
       "3         716  4.343284         55        0         2      7  \n",
       "4         122  5.473684          6        0         0      0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:51:18.963095Z",
     "start_time": "2019-03-09T13:51:18.958155Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_processing(df):\n",
    "    \n",
    "    ## Lower case\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    ## Removing Punctuation\n",
    "    df['text'] = df['text'].str.replace('[^\\w\\s]','')\n",
    "    ## Removing Stopwords\n",
    "    df['text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n",
    "    ## Spelling correction (time-required)\n",
    "#     df['text'] = df['text'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:51:38.321906Z",
     "start_time": "2019-03-09T13:51:29.029830Z"
    }
   },
   "outputs": [],
   "source": [
    "pre_processing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:51:40.256000Z",
     "start_time": "2019-03-09T13:51:40.248873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    student back neck constant strain dr serrick m...\n",
       "1    stayed football game university phoenix stadiu...\n",
       "2    good salads generous portions either get mexic...\n",
       "3    experience company growing awesome least memor...\n",
       "4    easiest furniture purchase great price deliver...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rare words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:52:55.065012Z",
     "start_time": "2019-03-09T13:52:53.019616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syed               1\n",
       "entropy            1\n",
       "safetydrwellish    1\n",
       "waterhole          1\n",
       "terres             1\n",
       "looot              1\n",
       "troublemade        1\n",
       "onback             1\n",
       "chipsguac          1\n",
       "swizzling          1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(df['text']).split()).value_counts()[-10:]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:53:02.574925Z",
     "start_time": "2019-03-09T13:52:59.869826Z"
    }
   },
   "outputs": [],
   "source": [
    "freq = list(freq.index)\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:53:20.376791Z",
     "start_time": "2019-03-09T13:53:03.833839Z"
    }
   },
   "source": [
    "tokenizer1 = Tokenizer(num_words=1000)\n",
    "tokenizer1.fit_on_texts(df['text'])\n",
    "vocab = tokenizer1.word_index\n",
    "\n",
    "x_train_word_ids = tokenizer1.texts_to_sequences(train['text'])\n",
    "x_test_word_ids = tokenizer1.texts_to_sequences(test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:53:23.094971Z",
     "start_time": "2019-03-09T13:53:22.232739Z"
    }
   },
   "source": [
    "x_train_padded_seqs = pad_sequences(x_train_word_ids, maxlen=100)\n",
    "x_test_padded_seqs = pad_sequences(x_test_word_ids, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:53:55.064695Z",
     "start_time": "2019-03-09T13:53:24.772125Z"
    }
   },
   "source": [
    "vectorizer = CountVectorizer(max_features=100, lowercase=True, analyzer='word',\n",
    "                             stop_words='english', ngram_range=(1, 2))\n",
    "\n",
    "X_train = vectorizer.fit_transform(train['text']).toarray()\n",
    "y = Train_y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:55:03.591773Z",
     "start_time": "2019-03-09T13:54:31.059626Z"
    }
   },
   "outputs": [],
   "source": [
    "df['text'] = [word_tokenize(entry) for entry in df['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T14:40:07.619307Z",
     "start_time": "2019-03-10T14:40:07.615641Z"
    }
   },
   "outputs": [],
   "source": [
    "tag_map = defaultdict(lambda : wordnet.NOUN)\n",
    "tag_map['J'] = wordnet.ADJ\n",
    "tag_map['V'] = wordnet.VERB\n",
    "tag_map['R'] = wordnet.ADV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T14:39:58.414043Z",
     "start_time": "2019-03-10T14:39:58.408338Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatization(df):\n",
    "    for index,entry in enumerate(df['text']):\n",
    "        # Declaring Empty List to store the words that follow the rules for this step\n",
    "        Final_words = []\n",
    "        # Initializing WordNetLemmatizer()\n",
    "        word_Lemmatized = WordNetLemmatizer()\n",
    "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "        for word, tag in pos_tag(entry):\n",
    "            # Below condition is to check for Stop words and consider only alphabets\n",
    "            if word not in stopwords.words('english') and word.isalpha():\n",
    "                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                Final_words.append(word_Final)\n",
    "        # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "        df.loc[index,'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-09T14:56:54.482Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatization(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"pre_processed_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T03:27:29.575073Z",
     "start_time": "2019-03-11T03:27:27.908705Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./pre_processed_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T03:27:29.631196Z",
     "start_time": "2019-03-11T03:27:29.593031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>hastags</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "      <th>text_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['student', 'back', 'neck', 'constant', 'strai...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>48</td>\n",
       "      <td>295</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>['student', 'back', 'neck', 'constant', 'strai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['stayed', 'football', 'game', 'university', '...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37</td>\n",
       "      <td>218</td>\n",
       "      <td>4.918919</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['stay', 'football', 'game', 'university', 'ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['good', 'salads', 'generous', 'portions', 'ei...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19</td>\n",
       "      <td>106</td>\n",
       "      <td>4.631579</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>['good', 'salad', 'generous', 'portion', 'eith...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['experience', 'company', 'growing', 'awesome'...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>134</td>\n",
       "      <td>716</td>\n",
       "      <td>4.343284</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>['experience', 'company', 'grow', 'awesome', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['easiest', 'furniture', 'purchase', 'great', ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19</td>\n",
       "      <td>122</td>\n",
       "      <td>5.473684</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['easy', 'furniture', 'purchase', 'great', 'pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>['great', 'hotel', 'clean', 'dont', 'like', 'p...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54</td>\n",
       "      <td>293</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['great', 'hotel', 'clean', 'dont', 'like', 'p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>['café', 'coffee', 'actually', 'important', 'w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>85</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['café', 'coffee', 'actually', 'important', 'b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>['impressed', 'ever', 'wanting', 'try', 'place...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>137</td>\n",
       "      <td>683</td>\n",
       "      <td>3.992701</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>['impressed', 'ever', 'want', 'try', 'place', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>['first', 'night', 'vegas', 'wanted', 'somethi...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>73</td>\n",
       "      <td>382</td>\n",
       "      <td>4.246575</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['first', 'night', 'vega', 'want', 'something'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>['21', 'reviews', '12', '1star', 'variety', 'c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>405</td>\n",
       "      <td>2318</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>['review', 'variety', 'coincidence', 'shock', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>['place', 'used', 'amazing', 'amazing', 'servi...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>142</td>\n",
       "      <td>766</td>\n",
       "      <td>4.328671</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['place', 'use', 'amaze', 'amazing', 'service'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>['best', 'fried', 'chicken', 'ever', 'town', '...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>38</td>\n",
       "      <td>189</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>['best', 'fry', 'chicken', 'ever', 'town', 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>['early', 'bird', 'specials', '11am', '10am', ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>41</td>\n",
       "      <td>238</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>['early', 'bird', 'special', 'forget', 'soup',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>['really', 'like', 'place', 'definitely', 'per...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>114</td>\n",
       "      <td>579</td>\n",
       "      <td>4.043860</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>['really', 'like', 'place', 'definitely', 'per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>['little', 'disappointed', '20oz', 'bonein', '...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>124</td>\n",
       "      <td>691</td>\n",
       "      <td>4.512000</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>['little', 'disappointed', 'bonein', 'ribeye',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>['whoever', 'picked', 'name', 'convenient', 'l...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>115</td>\n",
       "      <td>640</td>\n",
       "      <td>4.539130</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['whoever', 'pick', 'name', 'convenient', 'lit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>['extremely', 'extremely', 'super', 'nice', 's...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>38</td>\n",
       "      <td>252</td>\n",
       "      <td>5.048780</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['extremely', 'extremely', 'super', 'nice', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>['got', 'stir', 'fried', 'sticky', 'rice', 'gr...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>57</td>\n",
       "      <td>315</td>\n",
       "      <td>4.491228</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['get', 'stir', 'fry', 'sticky', 'rice', 'gran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>['really', 'disappointing', 'limited', 'select...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39</td>\n",
       "      <td>238</td>\n",
       "      <td>5.128205</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['really', 'disappointing', 'limited', 'select...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>['glad', 'gave', 'guys', 'second', 'chance', '...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>86</td>\n",
       "      <td>444</td>\n",
       "      <td>4.273810</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>['glad', 'give', 'guys', 'second', 'chance', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>['best', 'customer', 'service', 'ever', 'exper...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>154</td>\n",
       "      <td>857</td>\n",
       "      <td>4.551948</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>['best', 'customer', 'service', 'ever', 'exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>['ordered', 'go', 'waited', 'seems', 'like', '...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>78</td>\n",
       "      <td>409</td>\n",
       "      <td>4.256410</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>['order', 'go', 'waited', 'seem', 'like', 'lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>['clean', 'well', 'organized', 'friendly', 'st...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>35</td>\n",
       "      <td>205</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['clean', 'well', 'organize', 'friendly', 'sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>['love', 'concept', 'behind', 'everybody', 'in...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>178</td>\n",
       "      <td>1039</td>\n",
       "      <td>4.766667</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>['love', 'concept', 'behind', 'everybody', 'in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>['1', 'best', 'healthy', 'food', 'downtown', '...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>40</td>\n",
       "      <td>253</td>\n",
       "      <td>4.813953</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>['best', 'healthy', 'food', 'downtown', 'food'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>['place', 'awesome', 'food', 'delicious', 'sta...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>106</td>\n",
       "      <td>585</td>\n",
       "      <td>4.533333</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>['place', 'awesome', 'food', 'delicious', 'sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>['cookies', 'pretty', 'good', 'experience', 'f...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>68</td>\n",
       "      <td>370</td>\n",
       "      <td>4.455882</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>['cooky', 'pretty', 'good', 'experience', 'fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>['manicure', 'pedicure', 'done', 'came', 'home...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92</td>\n",
       "      <td>484</td>\n",
       "      <td>4.271739</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>['manicure', 'pedicure', 'do', 'come', 'home',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>['called', 'different', 'places', 'get', 'esti...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>79</td>\n",
       "      <td>423</td>\n",
       "      <td>4.367089</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>['call', 'different', 'place', 'get', 'estimat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>['hands', 'absolute', 'favorite', 'bar', 'las'...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>169</td>\n",
       "      <td>940</td>\n",
       "      <td>4.505882</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>['hand', 'absolute', 'favorite', 'bar', 'la', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109970</th>\n",
       "      <td>['bob', 'jim', 'great', 'people', 'friendly', ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>55</td>\n",
       "      <td>315</td>\n",
       "      <td>4.745455</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['bob', 'jim', 'great', 'people', 'friendly', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109971</th>\n",
       "      <td>['5', 'year', 'old', 'loves', 'endless', 'poss...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130</td>\n",
       "      <td>705</td>\n",
       "      <td>4.415385</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['year', 'old', 'love', 'endless', 'possibilit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109972</th>\n",
       "      <td>['deliciously', 'wonderful', 'place', 'korean'...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20</td>\n",
       "      <td>124</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>['deliciously', 'wonderful', 'place', 'korean'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109973</th>\n",
       "      <td>['holy', 'cow', 'butt', 'plugs', 'tails', 'mil...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>42</td>\n",
       "      <td>214</td>\n",
       "      <td>4.119048</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>['holy', 'cow', 'butt', 'plug', 'tail', 'milli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109974</th>\n",
       "      <td>['neighbors', 'guys', 'redo', 'driveway', 'gar...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>99</td>\n",
       "      <td>573</td>\n",
       "      <td>4.797980</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>['neighbor', 'guy', 'redo', 'driveway', 'garag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109975</th>\n",
       "      <td>['made', 'appointment', 'randy', 'phoenix', 'p...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>175</td>\n",
       "      <td>869</td>\n",
       "      <td>4.088235</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>['make', 'appointment', 'randy', 'phoenix', 'p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109976</th>\n",
       "      <td>['waffles', 'die', 'belgium', 'say', 'almost',...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>74</td>\n",
       "      <td>383</td>\n",
       "      <td>4.191781</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['waffle', 'die', 'belgium', 'say', 'almost', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109977</th>\n",
       "      <td>['husband', 'stayed', '102', '107', 'high', 'h...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>422</td>\n",
       "      <td>2164</td>\n",
       "      <td>4.125592</td>\n",
       "      <td>185</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>['husband', 'stay', 'high', 'hope', 'upon', 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109978</th>\n",
       "      <td>['couldnt', 'believe', 'tex', 'mex', 'far', 'h...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>90</td>\n",
       "      <td>466</td>\n",
       "      <td>4.144444</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>['couldnt', 'believe', 'tex', 'mex', 'far', 'h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109979</th>\n",
       "      <td>['learned', 'place', 'try', 'favorites', 'suga...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22</td>\n",
       "      <td>122</td>\n",
       "      <td>4.304348</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['learned', 'place', 'try', 'favorites', 'suga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109980</th>\n",
       "      <td>['amazing', 'ive', 'france', '4', 'times', 'iv...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>43</td>\n",
       "      <td>255</td>\n",
       "      <td>4.953488</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['amaze', 'ive', 'france', 'time', 'ive', 'liv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109981</th>\n",
       "      <td>['great', 'show', 'got', 'free', 'vip', 'ticke...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80</td>\n",
       "      <td>361</td>\n",
       "      <td>3.615385</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>['great', 'show', 'get', 'free', 'vip', 'ticke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109982</th>\n",
       "      <td>['revisited', 'wich', 'plans', 'kid', 'bad', '...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>192</td>\n",
       "      <td>965</td>\n",
       "      <td>4.052910</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>['revisit', 'wich', 'plan', 'kid', 'bad', 'day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109983</th>\n",
       "      <td>['food', 'absolutely', 'delicious', 'well', 'd...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14</td>\n",
       "      <td>89</td>\n",
       "      <td>5.428571</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['food', 'absolutely', 'delicious', 'well', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109984</th>\n",
       "      <td>['july', '29', '2015', 'bought', 'paid', 'arra...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>132</td>\n",
       "      <td>697</td>\n",
       "      <td>4.287879</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>['july', 'buy', 'pay', 'arranged', 'delivery',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109985</th>\n",
       "      <td>['decent', 'thai', 'food', 'ordered', 'takeout...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77</td>\n",
       "      <td>451</td>\n",
       "      <td>4.782051</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['decent', 'thai', 'food', 'order', 'takeout',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109986</th>\n",
       "      <td>['first', 'stop', 'need', 'buy', 'new', 'jeans...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>231</td>\n",
       "      <td>1274</td>\n",
       "      <td>4.381356</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>['first', 'stop', 'need', 'buy', 'new', 'jean'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109987</th>\n",
       "      <td>['avec', 'le', 'temps', 'de', 'moins', 'en', '...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35</td>\n",
       "      <td>180</td>\n",
       "      <td>4.171429</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['avec', 'le', 'temp', 'de', 'moins', 'en', 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109988</th>\n",
       "      <td>['seriously', 'love', 'place', 'best', 'cinnam...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44</td>\n",
       "      <td>244</td>\n",
       "      <td>4.568182</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['seriously', 'love', 'place', 'best', 'cinnam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109989</th>\n",
       "      <td>['place', 'dirty', 'service', 'pretty', 'bad',...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18</td>\n",
       "      <td>87</td>\n",
       "      <td>3.888889</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['place', 'dirty', 'service', 'pretty', 'bad',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109990</th>\n",
       "      <td>['clean', 'nice', 'atmosphere', 'quiet', 'dinn...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>27</td>\n",
       "      <td>149</td>\n",
       "      <td>4.555556</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['clean', 'nice', 'atmosphere', 'quiet', 'dinn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109991</th>\n",
       "      <td>['went', 'routine', 'check', 'horribly', 'roug...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66</td>\n",
       "      <td>372</td>\n",
       "      <td>4.651515</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['go', 'routine', 'check', 'horribly', 'rough'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109992</th>\n",
       "      <td>['entire', 'family', 'road', 'trip', 'pasadena...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>128</td>\n",
       "      <td>718</td>\n",
       "      <td>4.622047</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>['entire', 'family', 'road', 'trip', 'pasadena...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109993</th>\n",
       "      <td>['waited', 'hour', 'ultrasound', 'told', '3pm'...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>157</td>\n",
       "      <td>801</td>\n",
       "      <td>4.095541</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>['wait', 'hour', 'ultrasound', 'tell', 'show',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109994</th>\n",
       "      <td>['slow', 'time', 'lunch', 'service', 'painfull...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26</td>\n",
       "      <td>143</td>\n",
       "      <td>4.538462</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['slow', 'time', 'lunch', 'service', 'painfull...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109995</th>\n",
       "      <td>['picture', 'step', 'unassuming', 'door', 'cob...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>213</td>\n",
       "      <td>1196</td>\n",
       "      <td>4.600939</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['picture', 'step', 'unassuming', 'door', 'cob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109996</th>\n",
       "      <td>['im', 'feeling', 'generous', 'im', 'giving', ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>154</td>\n",
       "      <td>753</td>\n",
       "      <td>3.921569</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>['im', 'feel', 'generous', 'im', 'give', 'nail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109997</th>\n",
       "      <td>['driven', 'past', 'wondered', 'yelping', 'yes...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>123</td>\n",
       "      <td>618</td>\n",
       "      <td>4.032520</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>['drive', 'past', 'wonder', 'yelping', 'yester...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109998</th>\n",
       "      <td>['oh', 'dear', 'im', 'sure', 'say', 'place', '...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>281</td>\n",
       "      <td>1592</td>\n",
       "      <td>4.618375</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>['oh', 'dear', 'im', 'sure', 'say', 'place', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109999</th>\n",
       "      <td>['disappointed', 'service', 'bad', 'leave', 'h...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73</td>\n",
       "      <td>379</td>\n",
       "      <td>4.205479</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['disappointed', 'service', 'bad', 'leave', 'h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  stars  word_count  \\\n",
       "0       ['student', 'back', 'neck', 'constant', 'strai...    5.0          48   \n",
       "1       ['stayed', 'football', 'game', 'university', '...    3.0          37   \n",
       "2       ['good', 'salads', 'generous', 'portions', 'ei...    4.0          19   \n",
       "3       ['experience', 'company', 'growing', 'awesome'...    1.0         134   \n",
       "4       ['easiest', 'furniture', 'purchase', 'great', ...    5.0          19   \n",
       "5       ['great', 'hotel', 'clean', 'dont', 'like', 'p...    1.0          54   \n",
       "6       ['café', 'coffee', 'actually', 'important', 'w...    1.0          16   \n",
       "7       ['impressed', 'ever', 'wanting', 'try', 'place...    2.0         137   \n",
       "8       ['first', 'night', 'vegas', 'wanted', 'somethi...    5.0          73   \n",
       "9       ['21', 'reviews', '12', '1star', 'variety', 'c...    1.0         405   \n",
       "10      ['place', 'used', 'amazing', 'amazing', 'servi...    2.0         142   \n",
       "11      ['best', 'fried', 'chicken', 'ever', 'town', '...    5.0          38   \n",
       "12      ['early', 'bird', 'specials', '11am', '10am', ...    4.0          41   \n",
       "13      ['really', 'like', 'place', 'definitely', 'per...    5.0         114   \n",
       "14      ['little', 'disappointed', '20oz', 'bonein', '...    3.0         124   \n",
       "15      ['whoever', 'picked', 'name', 'convenient', 'l...    4.0         115   \n",
       "16      ['extremely', 'extremely', 'super', 'nice', 's...    5.0          38   \n",
       "17      ['got', 'stir', 'fried', 'sticky', 'rice', 'gr...    4.0          57   \n",
       "18      ['really', 'disappointing', 'limited', 'select...    2.0          39   \n",
       "19      ['glad', 'gave', 'guys', 'second', 'chance', '...    5.0          86   \n",
       "20      ['best', 'customer', 'service', 'ever', 'exper...    5.0         154   \n",
       "21      ['ordered', 'go', 'waited', 'seems', 'like', '...    4.0          78   \n",
       "22      ['clean', 'well', 'organized', 'friendly', 'st...    5.0          35   \n",
       "23      ['love', 'concept', 'behind', 'everybody', 'in...    5.0         178   \n",
       "24      ['1', 'best', 'healthy', 'food', 'downtown', '...    5.0          40   \n",
       "25      ['place', 'awesome', 'food', 'delicious', 'sta...    5.0         106   \n",
       "26      ['cookies', 'pretty', 'good', 'experience', 'f...    2.0          68   \n",
       "27      ['manicure', 'pedicure', 'done', 'came', 'home...    1.0          92   \n",
       "28      ['called', 'different', 'places', 'get', 'esti...    5.0          79   \n",
       "29      ['hands', 'absolute', 'favorite', 'bar', 'las'...    5.0         169   \n",
       "...                                                   ...    ...         ...   \n",
       "109970  ['bob', 'jim', 'great', 'people', 'friendly', ...    5.0          55   \n",
       "109971  ['5', 'year', 'old', 'loves', 'endless', 'poss...    3.0         130   \n",
       "109972  ['deliciously', 'wonderful', 'place', 'korean'...    5.0          20   \n",
       "109973  ['holy', 'cow', 'butt', 'plugs', 'tails', 'mil...    5.0          42   \n",
       "109974  ['neighbors', 'guys', 'redo', 'driveway', 'gar...    3.0          99   \n",
       "109975  ['made', 'appointment', 'randy', 'phoenix', 'p...    1.0         175   \n",
       "109976  ['waffles', 'die', 'belgium', 'say', 'almost',...    5.0          74   \n",
       "109977  ['husband', 'stayed', '102', '107', 'high', 'h...    2.0         422   \n",
       "109978  ['couldnt', 'believe', 'tex', 'mex', 'far', 'h...    4.0          90   \n",
       "109979  ['learned', 'place', 'try', 'favorites', 'suga...    4.0          22   \n",
       "109980  ['amazing', 'ive', 'france', '4', 'times', 'iv...    5.0          43   \n",
       "109981  ['great', 'show', 'got', 'free', 'vip', 'ticke...    4.0          80   \n",
       "109982  ['revisited', 'wich', 'plans', 'kid', 'bad', '...    3.0         192   \n",
       "109983  ['food', 'absolutely', 'delicious', 'well', 'd...    4.0          14   \n",
       "109984  ['july', '29', '2015', 'bought', 'paid', 'arra...    1.0         132   \n",
       "109985  ['decent', 'thai', 'food', 'ordered', 'takeout...    2.0          77   \n",
       "109986  ['first', 'stop', 'need', 'buy', 'new', 'jeans...    4.0         231   \n",
       "109987  ['avec', 'le', 'temps', 'de', 'moins', 'en', '...    2.0          35   \n",
       "109988  ['seriously', 'love', 'place', 'best', 'cinnam...    5.0          44   \n",
       "109989  ['place', 'dirty', 'service', 'pretty', 'bad',...    1.0          18   \n",
       "109990  ['clean', 'nice', 'atmosphere', 'quiet', 'dinn...    4.0          27   \n",
       "109991  ['went', 'routine', 'check', 'horribly', 'roug...    1.0          66   \n",
       "109992  ['entire', 'family', 'road', 'trip', 'pasadena...    5.0         128   \n",
       "109993  ['waited', 'hour', 'ultrasound', 'told', '3pm'...    2.0         157   \n",
       "109994  ['slow', 'time', 'lunch', 'service', 'painfull...    2.0          26   \n",
       "109995  ['picture', 'step', 'unassuming', 'door', 'cob...    5.0         213   \n",
       "109996  ['im', 'feeling', 'generous', 'im', 'giving', ...    2.0         154   \n",
       "109997  ['driven', 'past', 'wondered', 'yelping', 'yes...    4.0         123   \n",
       "109998  ['oh', 'dear', 'im', 'sure', 'say', 'place', '...    1.0         281   \n",
       "109999  ['disappointed', 'service', 'bad', 'leave', 'h...    1.0          73   \n",
       "\n",
       "        char_count  avg_word  stopwords  hastags  numerics  upper  \\\n",
       "0              295  5.166667         14        0         0      2   \n",
       "1              218  4.918919         13        0         0      0   \n",
       "2              106  4.631579          4        0         0      2   \n",
       "3              716  4.343284         55        0         2      7   \n",
       "4              122  5.473684          6        0         0      0   \n",
       "5              293  4.444444         18        0         0      0   \n",
       "6               85  4.375000          6        0         0      0   \n",
       "7              683  3.992701         60        0         0      6   \n",
       "8              382  4.246575         31        0         0      0   \n",
       "9             2318  4.666667        160        0         6     10   \n",
       "10             766  4.328671         62        0         0      1   \n",
       "11             189  4.000000         16        0         0      3   \n",
       "12             238  4.666667         11        0         1      2   \n",
       "13             579  4.043860         52        0         0      4   \n",
       "14             691  4.512000         47        0         0      8   \n",
       "15             640  4.539130         47        0         2      1   \n",
       "16             252  5.048780         10        0         0      0   \n",
       "17             315  4.491228         24        0         0      0   \n",
       "18             238  5.128205         14        0         0      0   \n",
       "19             444  4.273810         30        0         0      4   \n",
       "20             857  4.551948         65        1         1     16   \n",
       "21             409  4.256410         33        0         0      2   \n",
       "22             205  5.000000         11        0         0      1   \n",
       "23            1039  4.766667         79        0         0     10   \n",
       "24             253  4.813953          9        0         0      2   \n",
       "25             585  4.533333         40        0         0      7   \n",
       "26             370  4.455882         29        0         1      3   \n",
       "27             484  4.271739         35        0         0      4   \n",
       "28             423  4.367089         29        0         1      2   \n",
       "29             940  4.505882         71        0         0      6   \n",
       "...            ...       ...        ...      ...       ...    ...   \n",
       "109970         315  4.745455         21        0         1      1   \n",
       "109971         705  4.415385         50        0         2      1   \n",
       "109972         124  5.250000          5        0         0      3   \n",
       "109973         214  4.119048         12        0         0      3   \n",
       "109974         573  4.797980         41        0         0      5   \n",
       "109975         869  4.088235         75        0         4     12   \n",
       "109976         383  4.191781         36        0         0      1   \n",
       "109977        2164  4.125592        185        0         0     22   \n",
       "109978         466  4.144444         38        0         2      7   \n",
       "109979         122  4.304348          9        0         0      0   \n",
       "109980         255  4.953488         13        0         1      1   \n",
       "109981         361  3.615385         36        0         1      3   \n",
       "109982         965  4.052910         77        0         0      9   \n",
       "109983          89  5.428571          4        0         0      0   \n",
       "109984         697  4.287879         53        0         1      4   \n",
       "109985         451  4.782051         31        0         1      0   \n",
       "109986        1274  4.381356         91        0         0      9   \n",
       "109987         180  4.171429          4        0         0      0   \n",
       "109988         244  4.568182         16        0         0      1   \n",
       "109989          87  3.888889          8        0         0      0   \n",
       "109990         149  4.555556         11        0         0      1   \n",
       "109991         372  4.651515         32        0         1      1   \n",
       "109992         718  4.622047         42        0         2      4   \n",
       "109993         801  4.095541         74        0         0      8   \n",
       "109994         143  4.538462         11        0         0      0   \n",
       "109995        1196  4.600939         96        0         0      0   \n",
       "109996         753  3.921569         68        0         5      9   \n",
       "109997         618  4.032520         60        0         0      6   \n",
       "109998        1592  4.618375        111        0         3      6   \n",
       "109999         379  4.205479         33        0         0      1   \n",
       "\n",
       "                                               text_final  \n",
       "0       ['student', 'back', 'neck', 'constant', 'strai...  \n",
       "1       ['stay', 'football', 'game', 'university', 'ph...  \n",
       "2       ['good', 'salad', 'generous', 'portion', 'eith...  \n",
       "3       ['experience', 'company', 'grow', 'awesome', '...  \n",
       "4       ['easy', 'furniture', 'purchase', 'great', 'pr...  \n",
       "5       ['great', 'hotel', 'clean', 'dont', 'like', 'p...  \n",
       "6       ['café', 'coffee', 'actually', 'important', 'b...  \n",
       "7       ['impressed', 'ever', 'want', 'try', 'place', ...  \n",
       "8       ['first', 'night', 'vega', 'want', 'something'...  \n",
       "9       ['review', 'variety', 'coincidence', 'shock', ...  \n",
       "10      ['place', 'use', 'amaze', 'amazing', 'service'...  \n",
       "11      ['best', 'fry', 'chicken', 'ever', 'town', 'co...  \n",
       "12      ['early', 'bird', 'special', 'forget', 'soup',...  \n",
       "13      ['really', 'like', 'place', 'definitely', 'per...  \n",
       "14      ['little', 'disappointed', 'bonein', 'ribeye',...  \n",
       "15      ['whoever', 'pick', 'name', 'convenient', 'lit...  \n",
       "16      ['extremely', 'extremely', 'super', 'nice', 's...  \n",
       "17      ['get', 'stir', 'fry', 'sticky', 'rice', 'gran...  \n",
       "18      ['really', 'disappointing', 'limited', 'select...  \n",
       "19      ['glad', 'give', 'guys', 'second', 'chance', '...  \n",
       "20      ['best', 'customer', 'service', 'ever', 'exper...  \n",
       "21      ['order', 'go', 'waited', 'seem', 'like', 'lot...  \n",
       "22      ['clean', 'well', 'organize', 'friendly', 'sta...  \n",
       "23      ['love', 'concept', 'behind', 'everybody', 'in...  \n",
       "24      ['best', 'healthy', 'food', 'downtown', 'food'...  \n",
       "25      ['place', 'awesome', 'food', 'delicious', 'sta...  \n",
       "26      ['cooky', 'pretty', 'good', 'experience', 'fir...  \n",
       "27      ['manicure', 'pedicure', 'do', 'come', 'home',...  \n",
       "28      ['call', 'different', 'place', 'get', 'estimat...  \n",
       "29      ['hand', 'absolute', 'favorite', 'bar', 'la', ...  \n",
       "...                                                   ...  \n",
       "109970  ['bob', 'jim', 'great', 'people', 'friendly', ...  \n",
       "109971  ['year', 'old', 'love', 'endless', 'possibilit...  \n",
       "109972  ['deliciously', 'wonderful', 'place', 'korean'...  \n",
       "109973  ['holy', 'cow', 'butt', 'plug', 'tail', 'milli...  \n",
       "109974  ['neighbor', 'guy', 'redo', 'driveway', 'garag...  \n",
       "109975  ['make', 'appointment', 'randy', 'phoenix', 'p...  \n",
       "109976  ['waffle', 'die', 'belgium', 'say', 'almost', ...  \n",
       "109977  ['husband', 'stay', 'high', 'hope', 'upon', 'c...  \n",
       "109978  ['couldnt', 'believe', 'tex', 'mex', 'far', 'h...  \n",
       "109979  ['learned', 'place', 'try', 'favorites', 'suga...  \n",
       "109980  ['amaze', 'ive', 'france', 'time', 'ive', 'liv...  \n",
       "109981  ['great', 'show', 'get', 'free', 'vip', 'ticke...  \n",
       "109982  ['revisit', 'wich', 'plan', 'kid', 'bad', 'day...  \n",
       "109983  ['food', 'absolutely', 'delicious', 'well', 'd...  \n",
       "109984  ['july', 'buy', 'pay', 'arranged', 'delivery',...  \n",
       "109985  ['decent', 'thai', 'food', 'order', 'takeout',...  \n",
       "109986  ['first', 'stop', 'need', 'buy', 'new', 'jean'...  \n",
       "109987  ['avec', 'le', 'temp', 'de', 'moins', 'en', 'm...  \n",
       "109988  ['seriously', 'love', 'place', 'best', 'cinnam...  \n",
       "109989  ['place', 'dirty', 'service', 'pretty', 'bad',...  \n",
       "109990  ['clean', 'nice', 'atmosphere', 'quiet', 'dinn...  \n",
       "109991  ['go', 'routine', 'check', 'horribly', 'rough'...  \n",
       "109992  ['entire', 'family', 'road', 'trip', 'pasadena...  \n",
       "109993  ['wait', 'hour', 'ultrasound', 'tell', 'show',...  \n",
       "109994  ['slow', 'time', 'lunch', 'service', 'painfull...  \n",
       "109995  ['picture', 'step', 'unassuming', 'door', 'cob...  \n",
       "109996  ['im', 'feel', 'generous', 'im', 'give', 'nail...  \n",
       "109997  ['drive', 'past', 'wonder', 'yelping', 'yester...  \n",
       "109998  ['oh', 'dear', 'im', 'sure', 'say', 'place', '...  \n",
       "109999  ['disappointed', 'service', 'bad', 'leave', 'h...  \n",
       "\n",
       "[110000 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T03:27:31.112465Z",
     "start_time": "2019-03-11T03:27:31.094525Z"
    }
   },
   "outputs": [],
   "source": [
    "Train_x, Test_x, Train_y, Test_y = model_selection.train_test_split(\n",
    "    df['text_final'], df['stars'], test_size=(1/11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T03:27:32.345535Z",
     "start_time": "2019-03-11T03:27:32.332677Z"
    }
   },
   "outputs": [],
   "source": [
    "# Label\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "y_ohe_train = ohe.fit_transform(Train_y.values.reshape(-1, 1))\n",
    "y_ohe_test = ohe.fit_transform(Test_y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T03:27:34.593435Z",
     "start_time": "2019-03-11T03:27:34.589902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data  (100000,)\n",
      "train_labels  (100000,)\n",
      "____________________________________________________________________________________________________\n",
      "test_data  (10000,)\n",
      "test_labels  (10000,)\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"train_data \", Train_x.shape)\n",
    "print(\"train_labels \", Train_y.shape)\n",
    "print(\"_\"*100)\n",
    "print(\"test_data \", Test_x.shape)\n",
    "print(\"test_labels \", Test_y.shape)\n",
    "print(\"_\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T15:36:51.828361Z",
     "start_time": "2019-03-10T15:36:32.410210Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=100, lowercase=True, analyzer='word',\n",
    "                        stop_words='english', ngram_range=(1, 2))\n",
    "\n",
    "train_vect = tfidf.fit_transform(Train_x)\n",
    "test_vect = tfidf.fit_transform(Test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T15:37:37.986846Z",
     "start_time": "2019-03-10T15:37:37.954746Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amaze</th>\n",
       "      <th>area</th>\n",
       "      <th>ask</th>\n",
       "      <th>awesome</th>\n",
       "      <th>bad</th>\n",
       "      <th>bar</th>\n",
       "      <th>best</th>\n",
       "      <th>big</th>\n",
       "      <th>bit</th>\n",
       "      <th>burger</th>\n",
       "      <th>...</th>\n",
       "      <th>use</th>\n",
       "      <th>visit</th>\n",
       "      <th>wait</th>\n",
       "      <th>walk</th>\n",
       "      <th>want</th>\n",
       "      <th>wasnt</th>\n",
       "      <th>way</th>\n",
       "      <th>work</th>\n",
       "      <th>worth</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.263863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.491847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114035</td>\n",
       "      <td>0.126155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   amaze  area       ask  awesome  bad  bar      best  big  bit  burger  \\\n",
       "0    0.0   0.0  0.000000      0.0  0.0  0.0  0.263863  0.0  0.0     0.0   \n",
       "1    0.0   0.0  0.000000      0.0  0.0  0.0  0.000000  0.0  0.0     0.0   \n",
       "2    0.0   0.0  0.000000      0.0  0.0  0.0  0.000000  0.0  0.0     0.0   \n",
       "3    0.0   0.0  0.000000      0.0  0.0  0.0  0.000000  0.0  0.0     0.0   \n",
       "4    0.0   0.0  0.115995      0.0  0.0  0.0  0.000000  0.0  0.0     0.0   \n",
       "\n",
       "     ...     use  visit  wait      walk  want  wasnt  way  work     worth  \\\n",
       "0    ...     0.0    0.0   0.0  0.000000   0.0    0.0  0.0   0.0  0.000000   \n",
       "1    ...     0.0    0.0   0.0  0.000000   0.0    0.0  0.0   0.0  0.000000   \n",
       "2    ...     0.0    0.0   0.0  0.000000   0.0    0.0  0.0   0.0  0.000000   \n",
       "3    ...     0.0    0.0   0.0  0.491847   0.0    0.0  0.0   0.0  0.000000   \n",
       "4    ...     0.0    0.0   0.0  0.000000   0.0    0.0  0.0   0.0  0.114035   \n",
       "\n",
       "       year  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  0.000000  \n",
       "3  0.000000  \n",
       "4  0.126155  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the Model as a pandas DataFrame\n",
    "feature_names = tfidf.get_feature_names()\n",
    "train_vectdf = pd.DataFrame(train_vect, columns = feature_names)\n",
    "train_vectdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T15:33:58.991128Z",
     "start_time": "2019-03-10T15:33:58.969552Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 30\n",
    "pca = PCA(n_components)\n",
    "colname = []\n",
    "for i in range(0,n_components):\n",
    "    colname.append('PCA'+str(i+1))\n",
    "    \n",
    "principalComponents = pca.fit_transform(train_vect)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = [colname])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T14:23:15.429660Z",
     "start_time": "2019-03-10T14:23:06.988900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124556 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(100)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T13:58:27.415633Z",
     "start_time": "2019-03-10T13:57:52.266462Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# glove_input_file = './glove.6B.100d.txt'\n",
    "# word2vec_output_file = './glove.6B.100d.txt.word2vec'\n",
    "# glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "from gensim.models import KeyedVectors \n",
    "\n",
    "filename = './glove.6B.100d.txt.word2vec'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T14:03:30.037274Z",
     "start_time": "2019-03-10T14:03:30.033882Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 100000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T14:08:42.146164Z",
     "start_time": "2019-03-10T14:08:42.021800Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-5b5cf3409f38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_NUM_WORDS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mMAX_NUM_WORDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0membedding_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.random.random((MAX_NUM_WORDS + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index:\n",
    "    if i < MAX_NUM_WORDS:\n",
    "        embedding_vector = embeddings_index[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T13:58:27.560628Z",
     "start_time": "2019-03-10T13:58:21.689Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T15:40:03.702Z"
    }
   },
   "outputs": [],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=5, gamma='auto')\n",
    "SVM.fit(train_vect,Train_y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(test_vect)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_y)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T15:40:04.837Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T15:40:08.540Z"
    }
   },
   "outputs": [],
   "source": [
    "classifierGNB = GaussianNB()\n",
    "classifierGNB.fit(train_vect, Train_y)\n",
    "GNB_y_train_pred = classifierGNB.predict(train_vect)\n",
    "GNB_y_test_pred = classifierGNB.predict(test_vect)\n",
    "\n",
    "print(\"GaussianNB Accuracy (Train):\",metrics.accuracy_score(Train_y, GNB_y_train_pred))\n",
    "print(\"GaussianNB Accuracy (Test):\",metrics.accuracy_score(Test_y, GNB_y_test_pred))\n",
    "\n",
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(train_vect, Train_y)\n",
    "MGNB_y_train_pred = classifierGNB.predict(train_vect)\n",
    "MGNB_y_test_pred = clf.predict(test_vect)\n",
    "\n",
    "print(\"MultinomialNB Accuracy (Train):\",metrics.accuracy_score(Train_y, MGNB_y_train_pred))\n",
    "print(\"MultinomialNB Accuracy (Test):\",metrics.accuracy_score(Test_y, MGNB_y_test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T15:40:09.021Z"
    }
   },
   "outputs": [],
   "source": [
    "MGNB_y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T15:40:12.017Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_length = 100\n",
    "embedding_size = 100\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "dropout_rate = 0.5\n",
    "filters = 100\n",
    "kernel_sizes = [3, 6, 9]\n",
    "padding = 'valid'\n",
    "activation = 'relu'\n",
    "strides = 1\n",
    "pool_size = 2\n",
    "learning_rate = 0.1\n",
    "total_epoch = 10\n",
    "output_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-10T15:40:14.212Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating Callbacks\n",
    "# ModelCheckpoints is used to save the model after every epoch\n",
    "# EarlyStopping is used to stop training when the validation loss has not improved after 2 epochs\n",
    "# Tensorboard is used tovisualize dynamic graphs of the training and test metrics\n",
    "cbks = [callbacks.ModelCheckpoint(filepath='./checkpoint_model.h5', monitor='val_loss', save_best_only=True),\n",
    "            callbacks.EarlyStopping(monitor='val_loss', patience=2),callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T05:22:24.852277Z",
     "start_time": "2019-03-09T05:22:24.405076Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 100, 100)          10000000  \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 100, 100)          30100     \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 100, 100)          60100     \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 100, 100)          90100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1, 100)            0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 10,261,205\n",
      "Trainable params: 10,261,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Final Model Architecture\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=100000, output_dim=embedding_size,\n",
    "                    input_length=input_length, dropout=0.2))\n",
    "model.add(Conv1D(filters=filters, kernel_size=3, padding='same',\n",
    "                 activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model.add(Conv1D(filters=filters, kernel_size=6, padding='same',\n",
    "                 activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model.add(Conv1D(filters=filters, kernel_size=9, padding='same',\n",
    "                 activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model.add(MaxPooling1D(pool_size=((input_length-kernel_sizes[2])//strides+1)))\n",
    "# 1 layer of 100 units in the hidden layers of the LSTM cells\n",
    "model.add(LSTM(100, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model.add(Dense(output_size, activation='softmax',\n",
    "                kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T11:15:03.716880Z",
     "start_time": "2019-03-09T11:15:02.516117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 100, 100)          10000000  \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 100, 100)          30100     \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 100, 100)          60100     \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 100, 100)          90100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 1, 100)            0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 10,261,205\n",
      "Trainable params: 10,261,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Final model2 Architecture\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(input_dim=100000, output_dim=embedding_size,\n",
    "                     input_length=input_length, dropout=0.2))\n",
    "model2.add(Conv1D(filters=filters, kernel_size=3, padding='same',\n",
    "                  activation='relu'))\n",
    "model2.add(Conv1D(filters=filters, kernel_size=6, padding='same',\n",
    "                  activation='relu'))\n",
    "model2.add(Conv1D(filters=filters, kernel_size=9, padding='same',\n",
    "                  activation='relu'))\n",
    "model2.add(MaxPooling1D(pool_size=((input_length-kernel_sizes[2])//strides+1)))\n",
    "# 1 layer of 100 units in the hidden layers of the LSTM cells\n",
    "model2.add(LSTM(100))\n",
    "model2.add(Dense(output_size, activation='softmax'))\n",
    "\n",
    "optimizer = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "               optimizer=optimizer, metrics=['accuracy'])\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T09:30:13.346035Z",
     "start_time": "2019-03-09T09:30:12.993329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3363: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 100, 100)     10000       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100, 100)     0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 100, 100, 1)  0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 98, 1, 100)   30100       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 95, 1, 100)   60100       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 92, 1, 100)   90100       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 100)          0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 100)          0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 100)          0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 300)          0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 300)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 100)          30100       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 5)            505         dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 220,905\n",
      "Trainable params: 220,905\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# New model\n",
    "# YOUR CODE HERE\n",
    "x = Input(shape=(100, ))\n",
    "\n",
    "# embedding layer and dropout\n",
    "# YOUR CODE HERE\n",
    "e = Embedding(input_dim=100, output_dim=embedding_size, input_length=input_length)(x)\n",
    "e_d = Dropout(dropout_rate)(e)\n",
    "\n",
    "# construct the sequence tensor for CNN\n",
    "# YOUR CODE HERE\n",
    "e_d = Reshape((input_length, embedding_size, 1))(e_d)\n",
    "\n",
    "# CNN layers\n",
    "conv_blocks = []\n",
    "for kernel_size in kernel_sizes:\n",
    "    # YOUR CODE HERE\n",
    "    conv = Conv2D(filters=filters, kernel_size=(kernel_size, embedding_size), padding=padding, activation=activation, strides=(strides, strides), activity_regularizer=l1(0.001))(e_d)\n",
    "    maxpooling = MaxPool2D(pool_size=((input_length-kernel_size)//strides+1, 1))(conv)\n",
    "    faltten = Flatten()(maxpooling)\n",
    "    conv_blocks.append(faltten)\n",
    "\n",
    "# concatenate CNN results\n",
    "# YOUR CODE HERE\n",
    "c = Concatenate()(conv_blocks) if len(kernel_sizes) > 1 else conv_blocks[0]\n",
    "c_d = Dropout(dropout_rate)(c)\n",
    "\n",
    "# dense layer\n",
    "# YOUR CODE HERE\n",
    "L = LSTM(100)\n",
    "d = Dense(hidden_size, activation=activation, activity_regularizer=l1(0.001))(c_d)\n",
    "\n",
    "\n",
    "# output layer\n",
    "# YOUR CODE HERE\n",
    "y = Dense(output_size, activation='softmax', activity_regularizer=l1(0.001))(d)\n",
    "\n",
    "# build your own model\n",
    "# YOUR CODE HERE\n",
    "model_CNN = Model(x, y)\n",
    "\n",
    "# SGD optimizer with momentum\n",
    "optimizer = SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# compile model\n",
    "model_CNN.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model_CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T09:41:21.476678Z",
     "start_time": "2019-03-09T09:30:41.176002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 505s 5ms/step - loss: 19967.1972 - acc: 0.4395\n",
      "100000/100000 [==============================] - 117s 1ms/step\n",
      "10000/10000 [==============================] - 12s 1ms/step\n",
      "Training Loss: 1.531957049012184\n",
      " Training Accuracy: 0.43957999765872957\n",
      "Testng Loss: 1.5277322494983674\n",
      " Testing accuracy: 0.4405999979376793\n"
     ]
    }
   ],
   "source": [
    "model_CNN.fit(train_vect, y_ohe_train, epochs=1,\n",
    "          verbose=1, batch_size=32, callbacks=cbks)\n",
    "\n",
    "train_score = model_CNN.evaluate(\n",
    "    train_vect, y_ohe_train, batch_size=batch_size)\n",
    "test_score = model_CNN.evaluate(\n",
    "    test_vect, y_ohe_test, batch_size=batch_size)\n",
    "\n",
    "print('Training Loss: {}\\n Training Accuracy: {}\\n'\n",
    "      'Testng Loss: {}\\n Testing accuracy: {}'.format(\n",
    "          train_score[0], train_score[1],\n",
    "          test_score[0], test_score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T11:24:34.737690Z",
     "start_time": "2019-03-09T11:15:24.148196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 460s 5ms/step - loss: 1.4316 - acc: 0.4395\n",
      "100000/100000 [==============================] - 79s 787us/step\n",
      "10000/10000 [==============================] - 7s 698us/step\n",
      "Training Loss: 1.4426082400083542\n",
      " Training Accuracy: 0.43957999765872957\n",
      "Testng Loss: 1.438023966550827\n",
      " Testing accuracy: 0.4405999979376793\n"
     ]
    }
   ],
   "source": [
    "model2.fit(train_vect, y_ohe_train, epochs=1,\n",
    "          verbose=1, batch_size=32, callbacks=cbks)\n",
    "\n",
    "train_score = model2.evaluate(\n",
    "    train_vect, y_ohe_train, batch_size=batch_size)\n",
    "test_score = model2.evaluate(\n",
    "    test_vect, y_ohe_test, batch_size=batch_size)\n",
    "\n",
    "print('Training Loss: {}\\n Training Accuracy: {}\\n'\n",
    "      'Testng Loss: {}\\n Testing accuracy: {}'.format(\n",
    "          train_score[0], train_score[1],\n",
    "          test_score[0], test_score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:06:56.178508Z",
     "start_time": "2019-03-09T08:06:55.783587Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Sequential' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-d758278a1aa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfig1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Sequential' object is not subscriptable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visualizing Loss\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "%matplotlib inline\n",
    "\n",
    "fig1 = plt.figure()\n",
    "plt.plot(model.model['loss'],'r',linewidth=3.0)\n",
    "plt.legend(['Training Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('CNN Loss',fontsize=16)\n",
    "fig1.savefig('loss_cnn.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T08:06:56.343219Z",
     "start_time": "2019-03-09T08:06:56.308303Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Sequential' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-ff84f6b0103f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Training Accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Sequential' object is not subscriptable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visualizing accuracy\n",
    "\n",
    "fig2 = plt.figure()\n",
    "plt.plot(model.model['acc'], 'r', linewidth=3.0)\n",
    "plt.legend(['Training Accuracy'], fontsize=18)\n",
    "plt.xlabel('Epochs ', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.title('CNN Accuracy', fontsize=16)\n",
    "fig2.savefig('accuracy_cnn.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T09:43:17.218492Z",
     "start_time": "2019-03-09T09:43:05.455276Z"
    }
   },
   "outputs": [],
   "source": [
    "# predicting\n",
    "test_pre = model_CNN.predict(test_vect, batch_size=batch_size)\n",
    "# sub_df = pd.DataFrame()\n",
    "# sub_df[\"review_id\"] = test_id_list\n",
    "# sub_df[\"pre\"] = test_pre\n",
    "# sub_df.to_csv(\"pre.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T09:43:18.510582Z",
     "start_time": "2019-03-09T09:43:18.485734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11774056, 0.08284722, 0.13442786, 0.22003506, 0.4449493 ],\n",
       "       [0.11774056, 0.08284722, 0.13442786, 0.22003506, 0.4449493 ],\n",
       "       [0.11774056, 0.08284722, 0.13442786, 0.22003506, 0.4449493 ],\n",
       "       ...,\n",
       "       [0.11774056, 0.08284722, 0.13442786, 0.22003506, 0.4449493 ],\n",
       "       [0.11774056, 0.08284722, 0.13442786, 0.22003506, 0.4449493 ],\n",
       "       [0.11774056, 0.08284722, 0.13442786, 0.22003506, 0.4449493 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (Intel, 2019)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": "9999",
    "lenType": "9999",
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "370px",
    "left": "977px",
    "right": "7px",
    "top": "75px",
    "width": "388px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
