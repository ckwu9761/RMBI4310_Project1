{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T05:58:20.166264Z",
     "start_time": "2019-03-07T05:58:20.144457Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import random_projection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.layers import Input ,Dense, Dropout, Activation, LSTM\n",
    "from keras.layers import Conv1D, Convolution2D, MaxPooling2D, Flatten, Reshape, BatchNormalization, Concatenate\n",
    "from keras.models import Sequential\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "\n",
    "# -------------- Helper Functions --------------\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize\n",
    "    e.g.\n",
    "    Input: 'It is a nice day. I am happy.'\n",
    "    Output: ['it', 'is', 'a', 'nice', 'day', 'i', 'am', 'happy']\n",
    "    '''\n",
    "    tokens = []\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and not word.isnumeric():\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_sequence(data, seq_length, vocab_dict):\n",
    "    '''\n",
    "    :param data: a list of words, type: list\n",
    "    :param seq_length: the length of sequences,, type: int\n",
    "    :param vocab_dict: a dict from words to indices, type: dict\n",
    "    return a dense sequence matrix whose elements are indices of words,\n",
    "    '''\n",
    "    data_matrix = np.zeros((len(data), seq_length), dtype=int)\n",
    "    for i, doc in enumerate(data):\n",
    "        for j, word in enumerate(doc):\n",
    "            # YOUR CODE HERE\n",
    "            if j == seq_length:\n",
    "                break\n",
    "            word_idx = vocab_dict.get(word, 1) # 1 means the unknown word\n",
    "            data_matrix[i, j] = word_idx\n",
    "    return data_matrix\n",
    "\n",
    "\n",
    "def read_data(file_name, input_length, vocab=None):\n",
    "    \"\"\"\n",
    "    https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "    df['words'] = df['text'].apply(tokenize)\n",
    "\n",
    "    if vocab is None:\n",
    "        vocab = set()\n",
    "        for i in range(len(df)):\n",
    "            for word in df.iloc[i]['words']:\n",
    "                vocab.add(word)\n",
    "    vocab_dict = dict()\n",
    "    vocab_dict['<pad>'] = 0 # 0 means the padding signal\n",
    "    vocab_dict['<unk>'] = 1 # 1 means the unknown word\n",
    "    vocab_size = 2\n",
    "    for v in vocab:\n",
    "        vocab_dict[v] = vocab_size\n",
    "        vocab_size += 1\n",
    "\n",
    "    data_matrix = get_sequence(df['words'], input_length, vocab_dict)\n",
    "    stars = df['stars'].apply(int) - 1\n",
    "    return df['review_id'], stars, data_matrix, vocab\n",
    "# ----------------- End of Helper Functions-----------------\n",
    "\n",
    "\n",
    "def load_data(input_length):\n",
    "     # Load training data and vocab\n",
    "    train_id_list, train_data_label, train_data_matrix, vocab = read_data(\"data/train.csv\", input_length)\n",
    "    K = max(train_data_label)+1  # labels begin with 0\n",
    "\n",
    "    # Load testing data\n",
    "    test_id_list, _, test_data_matrix, _ = read_data(\"data/test.csv\", input_length, vocab=vocab)\n",
    "    test_data_label = pd.read_csv(\"data/valid.csv\")['stars'] - 1\n",
    "    \n",
    "    print(\"Vocabulary Size:\", len(vocab))\n",
    "    print(\"Training Set Size:\", len(train_id_list))\n",
    "    print(\"Test Set Size:\", len(test_id_list))\n",
    "    print(\"Training Set Shape:\", train_data_matrix.shape)\n",
    "    print(\"Testing Set Shape:\", test_data_matrix.shape)\n",
    "\n",
    "    # Converts a class vector to binary class matrix.\n",
    "    # https://keras.io/utils/#to_categorical\n",
    "    train_data_label = keras.utils.to_categorical(train_data_label, num_classes=K)\n",
    "    test_data_label = keras.utils.to_categorical(test_data_label, num_classes=K)\n",
    "    return train_data_matrix, train_data_label, test_data_matrix, test_data_label, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T08:42:16.808829Z",
     "start_time": "2019-03-06T08:42:16.542435Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/valid.csv\")\n",
    "test_data_label = pd.read_csv(\"data/valid.csv\")['stars'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T09:29:03.546443Z",
     "start_time": "2019-03-06T09:29:02.253549Z"
    }
   },
   "outputs": [],
   "source": [
    "Train = pd.read_csv(\"data/train.csv\")\n",
    "Test = pd.read_csv(\"data/test.csv\")\n",
    "Valid = pd.read_csv(\"data/valid.csv\")\n",
    "\n",
    "#################################################################\n",
    "Train = Train.iloc[0:10000,:]\n",
    "#################################################################\n",
    "# Label\n",
    "Train_y = Train['stars']\n",
    "Test_y = Valid['stars']\n",
    "df = pd.concat([Train, Valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T05:55:50.800607Z",
     "start_time": "2019-03-07T05:55:50.721406Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "input_length = 30\n",
    "embedding_size = 100\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "dropout_rate = 0.5\n",
    "filters = 100\n",
    "kernel_sizes = [3, 4, 5]\n",
    "padding = 'valid'\n",
    "activation = 'relu'\n",
    "strides = 1\n",
    "pool_size = 2\n",
    "learning_rate = 0.1\n",
    "total_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T08:44:55.551608Z",
     "start_time": "2019-03-06T08:42:16.881282Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 114655\n",
      "Training Set Size: 100000\n",
      "Test Set Size: 10000\n",
      "Training Set Shape: (100000, 30)\n",
      "Testing Set Shape: (10000, 30)\n"
     ]
    }
   ],
   "source": [
    "train_data_matrix, train_data_label, test_data_matrix, test_data_label, vocab = load_data(input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T09:54:46.844816Z",
     "start_time": "2019-03-06T09:54:46.840018Z"
    }
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "train_data_matrix = train_data_matrix[0:10000]\n",
    "train_data_label = train_data_label[1:10000]\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T09:19:48.013382Z",
     "start_time": "2019-03-06T09:19:48.007051Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data shape\n",
    "N = train_data_matrix.shape[0]\n",
    "K = train_data_label.shape[1]\n",
    "\n",
    "input_size = len(vocab) + 2\n",
    "output_size = K\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T05:58:23.292760Z",
     "start_time": "2019-03-07T05:58:23.068995Z"
    }
   },
   "outputs": [],
   "source": [
    "# New model\n",
    "# YOUR CODE HERE\n",
    "x = Input(shape=(input_length, ))\n",
    "\n",
    "# embedding layer and dropout\n",
    "# YOUR CODE HERE\n",
    "e = Embedding(input_dim=input_size, output_dim=embedding_size, input_length=input_length)(x)\n",
    "e_d = Dropout(dropout_rate)(e)\n",
    "\n",
    "# construct the sequence tensor for CNN\n",
    "# YOUR CODE HERE\n",
    "e_d = Reshape((input_length, embedding_size, 1))(e_d)\n",
    "\n",
    "# CNN layers\n",
    "conv_blocks = []\n",
    "for kernel_size in kernel_sizes:\n",
    "    # YOUR CODE HERE\n",
    "    conv = Conv2D(filters=filters, kernel_size=(kernel_size, embedding_size), padding=padding, activation=activation, strides=(strides, strides))(e_d)\n",
    "    maxpooling = MaxPool2D(pool_size=((input_length-kernel_size)//strides+1, 1))(conv)\n",
    "    faltten = Flatten()(maxpooling)\n",
    "    conv_blocks.append(faltten)\n",
    "\n",
    "# concatenate CNN results\n",
    "# YOUR CODE HERE\n",
    "c = Concatenate()(conv_blocks) if len(kernel_sizes) > 1 else conv_blocks[0]\n",
    "c_d = Dropout(dropout_rate)(c)\n",
    "\n",
    "# dense layer\n",
    "# YOUR CODE HERE\n",
    "d = Dense(hidden_size, activation=activation)(c_d)\n",
    "\n",
    "# output layer\n",
    "# YOUR CODE HERE\n",
    "y = Dense(output_size, activation='softmax')(d)\n",
    "\n",
    "# build your own model\n",
    "# YOUR CODE HERE\n",
    "model_CNN = Model(x, y)\n",
    "\n",
    "# SGD optimizer with momentum\n",
    "optimizer = SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# compile model\n",
    "model_CNN.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T06:01:08.168887Z",
     "start_time": "2019-03-07T05:58:38.353870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9997/9997 [==============================] - 15s 2ms/step - loss: 1.4274 - acc: 0.4375\n",
      "Epoch 2/10\n",
      "9997/9997 [==============================] - 15s 1ms/step - loss: 1.4140 - acc: 0.4379\n",
      "Epoch 3/10\n",
      "9997/9997 [==============================] - 14s 1ms/step - loss: 1.3992 - acc: 0.4379\n",
      "Epoch 4/10\n",
      "9997/9997 [==============================] - 16s 2ms/step - loss: 1.3283 - acc: 0.4587\n",
      "Epoch 5/10\n",
      "9997/9997 [==============================] - 14s 1ms/step - loss: 1.2705 - acc: 0.4884\n",
      "Epoch 6/10\n",
      "9997/9997 [==============================] - 16s 2ms/step - loss: 1.1985 - acc: 0.5152\n",
      "Epoch 7/10\n",
      "9997/9997 [==============================] - 14s 1ms/step - loss: 1.1170 - acc: 0.5372\n",
      "Epoch 8/10\n",
      "9997/9997 [==============================] - 14s 1ms/step - loss: 1.0612 - acc: 0.5588\n",
      "Epoch 9/10\n",
      "9997/9997 [==============================] - 15s 1ms/step - loss: 1.0102 - acc: 0.5817\n",
      "Epoch 10/10\n",
      "9997/9997 [==============================] - 13s 1ms/step - loss: 0.9589 - acc: 0.5959\n",
      "9997/9997 [==============================] - 1s 150us/step\n",
      "10000/10000 [==============================] - 1s 133us/step\n",
      "Training Loss: 0.8139531481167048\n",
      " Training Accuracy: 0.6515954822149828\n",
      "Testng Loss: 2.32424614071846\n",
      " Testing accuracy: 0.31980000153183935\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "model_CNN.fit(train_data_matrix, train_data_label, epochs=total_epoch, batch_size=batch_size)\n",
    "# testing\n",
    "train_score = model_CNN.evaluate(train_data_matrix, train_data_label, batch_size=batch_size)\n",
    "test_score = model_CNN.evaluate(test_data_matrix, test_data_label, batch_size=batch_size)\n",
    "\n",
    "print('Training Loss: {}\\n Training Accuracy: {}\\n'\n",
    "      'Testng Loss: {}\\n Testing accuracy: {}'.format(\n",
    "          train_score[0], train_score[1],\n",
    "          test_score[0], test_score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T09:50:43.277232Z",
     "start_time": "2019-03-06T09:50:42.969256Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# New model_LSTM\n",
    "model_LSTM = Sequential()\n",
    "\n",
    "# embedding layer and dropout\n",
    "# YOUR CODE HERE\n",
    "model_LSTM.add(Embedding(input_dim=input_size,\n",
    "                         output_dim=embedding_size, input_length=input_length))\n",
    "model_LSTM.add(Dropout(dropout_rate))\n",
    "\n",
    "# LSTM layer\n",
    "# YOUR CODE HERE\n",
    "model_LSTM.add(LSTM(units=hidden_size))\n",
    "\n",
    "# output layer\n",
    "# YOUR CODE HERE\n",
    "model_LSTM.add(Dense(K, activation='softmax'))\n",
    "\n",
    "# SGD optimizer with momentum\n",
    "optimizer = SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# compile model_LSTM\n",
    "model_LSTM.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T09:56:33.854617Z",
     "start_time": "2019-03-06T09:54:56.640416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9997/9997 [==============================] - 9s 939us/step - loss: 1.4104 - acc: 0.4379\n",
      "Epoch 2/10\n",
      "9997/9997 [==============================] - 9s 947us/step - loss: 1.4012 - acc: 0.4385\n",
      "Epoch 3/10\n",
      "9997/9997 [==============================] - 9s 921us/step - loss: 1.3928 - acc: 0.4417\n",
      "Epoch 4/10\n",
      "9997/9997 [==============================] - 9s 937us/step - loss: 1.4055 - acc: 0.4402\n",
      "Epoch 5/10\n",
      "9997/9997 [==============================] - 9s 930us/step - loss: 1.3746 - acc: 0.4521\n",
      "Epoch 6/10\n",
      "9997/9997 [==============================] - 9s 946us/step - loss: 1.2865 - acc: 0.4942\n",
      "Epoch 7/10\n",
      "9997/9997 [==============================] - 10s 962us/step - loss: 1.1763 - acc: 0.5247\n",
      "Epoch 8/10\n",
      "9997/9997 [==============================] - 10s 976us/step - loss: 1.0955 - acc: 0.5505\n",
      "Epoch 9/10\n",
      "9997/9997 [==============================] - 9s 944us/step - loss: 1.0173 - acc: 0.5859\n",
      "Epoch 10/10\n",
      "9997/9997 [==============================] - 9s 946us/step - loss: 0.9512 - acc: 0.6085\n",
      "9997/9997 [==============================] - 1s 144us/step\n",
      "10000/10000 [==============================] - 1s 127us/step\n",
      "Training Loss: 0.8509425757956097\n",
      " Training Accuracy: 0.6583975199295113\n",
      "Testng Loss: 2.266466431617737\n",
      " Testing accuracy: 0.2720000001788139\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "model_LSTM.fit(train_data_matrix, train_data_label, epochs=total_epoch, batch_size=batch_size)\n",
    "# testing\n",
    "train_score = model_LSTM.evaluate(train_data_matrix, train_data_label, batch_size=batch_size)\n",
    "test_score = model_LSTM.evaluate(test_data_matrix, test_data_label, batch_size=batch_size)\n",
    "\n",
    "print('Training Loss: {}\\n Training Accuracy: {}\\n'\n",
    "      'Testng Loss: {}\\n Testing accuracy: {}'.format(\n",
    "          train_score[0], train_score[1],\n",
    "          test_score[0], test_score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting\n",
    "test_pre = model.predict(test_data_matrix, batch_size=batch_size)\n",
    "sub_df = pd.DataFrame()\n",
    "sub_df[\"review_id\"] = test_id_list\n",
    "sub_df[\"pre\"] = test_pre\n",
    "sub_df.to_csv(\"pre.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "452px",
    "left": "1038px",
    "right": "1px",
    "top": "75px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
